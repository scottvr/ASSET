{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Notebook Generated by ChimeraCat\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generated by ChimeraCat\n",
        "    #  /\\___/\\  ChimeraCat\n",
        "    # ( o   o )  Modular Python Fusion\n",
        "    # (  =^=  ) \n",
        "    #  (______)  Generated: 2024-11-20 04:29:43\n",
        "    #\n",
        "    # Compression Level: full\n",
        "    \n",
        "# External imports\n",
        "import abc\n",
        "import dataclasses\n",
        "import datetime\n",
        "import enum\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import soundfile as sf\n",
        "import spleeter.separator\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import typing\n",
        "\n",
        "# Combined module code\n",
        "\n",
        "\n",
        "# From analysis\\artifacts\\base.py\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "@dataclass\n",
        "class ValidationMetrics:\n",
        "    \"\"\"Stores comparison metrics between original and processed audio\"\"\"\n",
        "    phase_coherence: float\n",
        "    frequency_response: Dict[str, float]\n",
        "    signal_to_noise: float\n",
        "    processing_time: float\n",
        "    memory_usage: float\n",
        "    \n",
        "    def as_dict(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            \"phase_coherence\": self.phase_coherence,\n",
        "            \"freq_response_mean\": np.mean(list(self.frequency_response.values())),\n",
        "            \"freq_response_std\": np.std(list(self.frequency_response.values())),\n",
        "            \"snr\": self.signal_to_noise,\n",
        "            \"processing_time\": self.processing_time,\n",
        "            \"memory_usage\": self.memory_usage\n",
        "        }\n",
        "\n",
        "class ArtifactProcessor(ABC):\n",
        "    \"\"\"Base class for different artifact processing approaches\"\"\"\n",
        "    \n",
        "    def __init__(self, sample_rate: int = 44100):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.frequency_bands = {\n",
        "            \"sub_bass\": (20, 60),\n",
        "            \"bass\": (60, 250),\n",
        "            \"low_mid\": (250, 500),\n",
        "            \"mid\": (500, 2000),\n",
        "            \"high_mid\": (2000, 4000),\n",
        "            \"presence\": (4000, 6000),\n",
        "            \"brilliance\": (6000, 20000)\n",
        "        }\n",
        "    \n",
        "    @abstractmethod\n",
        "    def process(self, \n",
        "                audio_segment: AudioSegment,\n",
        "                artifact_type: str) -> AudioSegment:\n",
        "        \"\"\"Process audio to remove specified artifact type\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def validate(self, \n",
        "                clean: AudioSegment,\n",
        "                processed: AudioSegment,\n",
        "                original_artifacts: AudioSegment) -> ValidationMetrics:\n",
        "        \"\"\"Calculate validation metrics comparing processed output to clean reference\"\"\"\n",
        "        \n",
        "        # Calculate phase coherence\n",
        "        phase_coherence = self._measure_phase_coherence(\n",
        "            clean.audio, processed.audio\n",
        "        )\n",
        "        \n",
        "        # Analyze frequency response\n",
        "        freq_response = self._analyze_frequency_response(\n",
        "            clean.audio, processed.audio\n",
        "        )\n",
        "        \n",
        "        # Calculate SNR using original artifacts as noise reference\n",
        "        snr = self._calculate_snr(\n",
        "            clean.audio,\n",
        "            processed.audio,\n",
        "            original_artifacts.audio\n",
        "        )\n",
        "        \n",
        "        return ValidationMetrics(\n",
        "            phase_coherence=phase_coherence,\n",
        "            frequency_response=freq_response,\n",
        "            signal_to_noise=snr,\n",
        "            processing_time=0.0,  # Set by wrapper\n",
        "            memory_usage=0.0  # Set by wrapper\n",
        "        )\n",
        "    \n",
        "    def _measure_phase_coherence(self,\n",
        "                               clean: np.ndarray,\n",
        "                               processed: np.ndarray) -> float:\n",
        "        \"\"\"Measure phase coherence between clean and processed audio\"\"\"\n",
        "        # Use STFT for phase analysis\n",
        "        n_fft = 2048\n",
        "        hop_length = 512\n",
        "        \n",
        "        clean_stft = librosa.stft(clean, n_fft=n_fft, hop_length=hop_length)\n",
        "        proc_stft = librosa.stft(processed, n_fft=n_fft, hop_length=hop_length)\n",
        "        \n",
        "        # Calculate phase difference\n",
        "        clean_phase = np.angle(clean_stft)\n",
        "        proc_phase = np.angle(proc_stft)\n",
        "        phase_diff = np.abs(clean_phase - proc_phase)\n",
        "        \n",
        "        # Return mean phase coherence (1 = perfect, 0 = random)\n",
        "        return float(np.mean(np.cos(phase_diff)))\n",
        "    \n",
        "    def _analyze_frequency_response(self,\n",
        "                                  clean: np.ndarray,\n",
        "                                  processed: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Analyze frequency response preservation in different bands\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        for band_name, (low, high) in self.frequency_bands.items():\n",
        "            # Filter both signals to band\n",
        "            clean_band = self._bandpass_filter(clean, low, high)\n",
        "            proc_band = self._bandpass_filter(processed, low, high)\n",
        "            \n",
        "            # Calculate RMS difference\n",
        "            clean_rms = np.sqrt(np.mean(clean_band ** 2))\n",
        "            proc_rms = np.sqrt(np.mean(proc_band ** 2))\n",
        "            \n",
        "            # Store ratio (1 = perfect preservation)\n",
        "            results[band_name] = proc_rms / clean_rms if clean_rms > 0 else 0.0\n",
        "            \n",
        "        return results\n",
        "    \n",
        "    def _bandpass_filter(self,\n",
        "                        audio: np.ndarray,\n",
        "                        low_freq: float,\n",
        "                        high_freq: float) -> np.ndarray:\n",
        "        \"\"\"Apply bandpass filter to audio\"\"\"\n",
        "        nyquist = self.sample_rate // 2\n",
        "        low = low_freq / nyquist\n",
        "        high = high_freq / nyquist\n",
        "        \n",
        "        # Design filter\n",
        "        b, a = scipy.signal.butter(4, [low, high], btype='band')\n",
        "        \n",
        "        # Apply filter\n",
        "        return scipy.signal.filtfilt(b, a, audio)\n",
        "    \n",
        "    def _calculate_snr(self,\n",
        "                      clean: np.ndarray,\n",
        "                      processed: np.ndarray,\n",
        "                      noise: np.ndarray) -> float:\n",
        "        \"\"\"Calculate signal-to-noise ratio\"\"\"\n",
        "        signal_power = np.mean(clean ** 2)\n",
        "        noise_power = np.mean((processed - clean) ** 2)\n",
        "        \n",
        "        return 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')\n",
        "\n",
        "class ControlNetProcessor(ArtifactProcessor):\n",
        "    \"\"\"ControlNet-based artifact processor\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: Optional[Path] = None):\n",
        "        super().__init__()\n",
        "        self.model = PhaseAwareControlNet(...)  # Initialize from earlier code\n",
        "        if model_path:\n",
        "            self.model.load_state_dict(torch.load(model_path))\n",
        "    \n",
        "    def process(self,\n",
        "                audio_segment: AudioSegment,\n",
        "                artifact_type: str) -> AudioSegment:\n",
        "        # Convert to spectrogram\n",
        "        spec = self._audio_to_spectrogram(audio_segment)\n",
        "        \n",
        "        # Process through ControlNet\n",
        "        processed_spec = self.model(spec)\n",
        "        \n",
        "        # Convert back to audio\n",
        "        return self._spectrogram_to_audio(processed_spec)\n",
        "\n",
        "class SignalProcessor(ArtifactProcessor):\n",
        "    \"\"\"Direct signal-domain processor\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def process(self,\n",
        "                audio_segment: AudioSegment,\n",
        "                artifact_type: str) -> AudioSegment:\n",
        "        # Implement direct signal processing approach\n",
        "        # (We'll flesh this out as we identify specific artifacts)\n",
        "        pass\n",
        "\n",
        "class HybridProcessor(ArtifactProcessor):\n",
        "    \"\"\"Hybrid approach combining latent and signal processing\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.controlnet = ControlNetProcessor()\n",
        "        self.signal = SignalProcessor()\n",
        "        \n",
        "    def process(self,\n",
        "                audio_segment: AudioSegment,\n",
        "                artifact_type: str) -> AudioSegment:\n",
        "        # Route to appropriate processor based on artifact type\n",
        "        # Or combine both approaches\n",
        "        pass\n",
        "\n",
        "def run_validation(\n",
        "    processor: ArtifactProcessor,\n",
        "    test_cases: List[Tuple[AudioSegment, AudioSegment, AudioSegment]],\n",
        "    artifact_types: List[str]\n",
        ") -> Dict[str, List[ValidationMetrics]]:\n",
        "    \"\"\"Run validation suite on processor\"\"\"\n",
        "    \n",
        "    results = {artifact_type: [] for artifact_type in artifact_types}\n",
        "    \n",
        "    for clean, artifacts, mixed in test_cases:\n",
        "        for artifact_type in artifact_types:\n",
        "            # Time and memory measurement wrapper\n",
        "            start_time = time.time()\n",
        "            start_mem = psutil.Process().memory_info().rss\n",
        "            \n",
        "            # Process audio\n",
        "            processed = processor.process(mixed, artifact_type)\n",
        "            \n",
        "            # Get metrics\n",
        "            metrics = processor.validate(clean, processed, artifacts)\n",
        "            metrics.processing_time = time.time() - start_time\n",
        "            metrics.memory_usage = (\n",
        "                psutil.Process().memory_info().rss - start_mem\n",
        "            ) / 1024 / 1024  # MB\n",
        "            \n",
        "            results[artifact_type].append(metrics)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# From analysis\\artifacts\\high_freq.py\n",
        "class HighFrequencyArtifactPreprocessor(nn.Module):\n",
        "    def __init__(self, threshold_freq: float = 11000, sample_rate: int = 44100):\n",
        "        super().__init__()\n",
        "        self.threshold_freq = threshold_freq\n",
        "        self.sample_rate = sample_rate\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: RGBA spectrogram tensor (B, 4, H, W)\n",
        "        Returns:\n",
        "            Single-channel attention map (B, 1, H, W)\n",
        "        \"\"\"\n",
        "        # Extract magnitude from blue channel\n",
        "        magnitude = x[:, 2:3]  # (B, 1, H, W)\n",
        "        \n",
        "        # Calculate frequency bins\n",
        "        freq_bins = torch.linspace(0, self.sample_rate/2, magnitude.shape[2])\n",
        "        \n",
        "        # Create frequency mask\n",
        "        mask = torch.ones_like(magnitude)\n",
        "        high_freq_idx = (freq_bins > self.threshold_freq).nonzero()\n",
        "        \n",
        "        if high_freq_idx.numel() > 0:\n",
        "            # Analyze high frequency content\n",
        "            high_freq_content = magnitude[:, :, high_freq_idx.squeeze():]\n",
        "            \n",
        "            # Detect potential artifacts using local statistics\n",
        "            mean = torch.mean(high_freq_content, dim=-1, keepdim=True)\n",
        "            std = torch.std(high_freq_content, dim=-1, keepdim=True)\n",
        "            \n",
        "            # Create attention map\n",
        "            attention = torch.sigmoid(\n",
        "                (high_freq_content - mean) / (std + 1e-6)\n",
        "            )\n",
        "            \n",
        "            # Insert back into full mask\n",
        "            mask[:, :, high_freq_idx.squeeze():] = attention\n",
        "        \n",
        "        return mask\n",
        "\n",
        "def generate_training_pair(\n",
        "    clean_audio: torch.Tensor,\n",
        "    separated_audio: torch.Tensor,\n",
        "    preprocessor: HighFrequencyArtifactPreprocessor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Generate training pair for ControlNet\n",
        "    Returns:\n",
        "        condition: Control signal from preprocessor\n",
        "        input_spec: Separated audio spectrogram\n",
        "        target_spec: Clean audio spectrogram\n",
        "    \"\"\"\n",
        "    # Convert both to spectrograms\n",
        "    clean_spec = audio_to_spectrogram(clean_audio)\n",
        "    sep_spec = audio_to_spectrogram(separated_audio)\n",
        "    \n",
        "    # Generate control signal\n",
        "    condition = preprocessor(sep_spec)\n",
        "    \n",
        "    return condition, sep_spec, clean_spec\n",
        "\n",
        "# From analysis\\artifacts\\preprocessor.py\n",
        "class HighFrequencyArtifactPreprocessor(nn.Module):\n",
        "    def __init__(self, threshold_freq: float = 11000, sample_rate: int = 44100):\n",
        "        super().__init__()\n",
        "        self.threshold_freq = threshold_freq\n",
        "        self.sample_rate = sample_rate\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: RGBA spectrogram tensor (B, 4, H, W)\n",
        "        Returns:\n",
        "            Single-channel attention map (B, 1, H, W)\n",
        "        \"\"\"\n",
        "        # Extract magnitude from blue channel\n",
        "        magnitude = x[:, 2:3]  # (B, 1, H, W)\n",
        "        \n",
        "        # Calculate frequency bins\n",
        "        freq_bins = torch.linspace(0, self.sample_rate/2, magnitude.shape[2])\n",
        "        \n",
        "        # Create frequency mask\n",
        "        mask = torch.ones_like(magnitude)\n",
        "        high_freq_idx = (freq_bins > self.threshold_freq).nonzero()\n",
        "        \n",
        "        if high_freq_idx.numel() > 0:\n",
        "            # Analyze high frequency content\n",
        "            high_freq_content = magnitude[:, :, high_freq_idx.squeeze():]\n",
        "            \n",
        "            # Detect potential artifacts using local statistics\n",
        "            mean = torch.mean(high_freq_content, dim=-1, keepdim=True)\n",
        "            std = torch.std(high_freq_content, dim=-1, keepdim=True)\n",
        "            \n",
        "            # Create attention map\n",
        "            attention = torch.sigmoid(\n",
        "                (high_freq_content - mean) / (std + 1e-6)\n",
        "            )\n",
        "            \n",
        "            # Insert back into full mask\n",
        "            mask[:, :, high_freq_idx.squeeze():] = attention\n",
        "        \n",
        "        return mask\n",
        "\n",
        "def generate_training_pair(\n",
        "    clean_audio: torch.Tensor,\n",
        "    separated_audio: torch.Tensor,\n",
        "    preprocessor: HighFrequencyArtifactPreprocessor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Generate training pair for ControlNet\n",
        "    Returns:\n",
        "        condition: Control signal from preprocessor\n",
        "        input_spec: Separated audio spectrogram\n",
        "        target_spec: Clean audio spectrogram\n",
        "    \"\"\"\n",
        "    # Convert both to spectrograms\n",
        "    clean_spec = audio_to_spectrogram(clean_audio)\n",
        "    sep_spec = audio_to_spectrogram(separated_audio)\n",
        "    \n",
        "    # Generate control signal\n",
        "    condition = preprocessor(sep_spec)\n",
        "    \n",
        "    return condition, sep_spec, clean_spec\n",
        "\n",
        "# From core\\audio.py\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import librosa\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class AudioSegment:\n",
        "    \"\"\"Data class for audio segments with their metadata\"\"\"\n",
        "    audio: np.ndarray\n",
        "    sample_rate: int = 44100\n",
        "    start_time: float = 0.0\n",
        "    duration: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def is_stereo(self) -> bool:\n",
        "        stereo = len(self.audio.shape) == 2 and (\n",
        "            self.audio.shape[0] == 2 or self.audio.shape[1] == 2\n",
        "        )\n",
        "        print(f\"is_stereo check - shape: {self.audio.shape}, result: {stereo}\")\n",
        "        return stereo\n",
        "\n",
        "    @property\n",
        "    def is_mono(self) -> bool:\n",
        "        mono = len(self.audio.shape) == 1 or (\n",
        "            len(self.audio.shape) == 2 and (\n",
        "                self.audio.shape[0] == 1 or self.audio.shape[1] == 1\n",
        "            )\n",
        "        )\n",
        "        print(f\"is_mono check - shape: {self.audio.shape}, result: {mono}\")\n",
        "        return mono\n",
        "\n",
        "    def to_mono(self) -> 'AudioSegment':\n",
        "        \"\"\"Convert to mono if stereo\"\"\"\n",
        "        print(f\"to_mono - input shape: {self.audio.shape}\")\n",
        "        \n",
        "        if self.is_mono:\n",
        "            print(\"Already mono, returning as is\")\n",
        "            return self\n",
        "            \n",
        "        # Handle different stereo formats\n",
        "        if len(self.audio.shape) == 2:\n",
        "            if self.audio.shape[0] == 2:\n",
        "                # (2, samples) format\n",
        "                mono_audio = librosa.to_mono(self.audio)\n",
        "            elif self.audio.shape[1] == 2:\n",
        "                # (samples, 2) format\n",
        "                mono_audio = librosa.to_mono(self.audio.T)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected audio shape: {self.audio.shape}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Cannot convert shape {self.audio.shape} to mono\")\n",
        "            \n",
        "        print(f\"to_mono - output shape: {mono_audio.shape}\")\n",
        "        \n",
        "        return AudioSegment(\n",
        "            audio=mono_audio,\n",
        "            sample_rate=self.sample_rate,\n",
        "            start_time=self.start_time,\n",
        "            duration=self.duration\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def duration_seconds(self) -> float:\n",
        "        \"\"\"Get duration in seconds based on audio shape and sample rate\"\"\"\n",
        "        if self.is_stereo:\n",
        "            return self.audio.shape[1] / self.sample_rate\n",
        "        return len(self.audio) / self.sample_rate\n",
        "\n",
        "# From core\\config.py\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum, auto\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "class SeparatorBackend(Enum):\n",
        "    SPLEETER = auto()\n",
        "    DEMUCS = auto()\n",
        "    MDX = auto()\n",
        "\n",
        "@dataclass\n",
        "class SeparationProfile:\n",
        "    \"\"\"Processing profile configuration\"\"\"\n",
        "    backend: SeparatorBackend\n",
        "    preserve_high_freq: bool = False\n",
        "    target_sample_rate: int = 44100\n",
        "    min_segment_length: float = 5.0\n",
        "    \n",
        "    # Enhancement settings\n",
        "    use_phase_aware_controlnet: bool = False\n",
        "    use_high_freq_processor: bool = True\n",
        "    artifact_reduction_config: Optional[ProcessingConfig] = None\n",
        "    controlnet_model_path: Optional[Path] = None\n",
        "\n",
        "# From core\\types.py\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Any\n",
        "# from .audio import AudioSegment  # Original relative import\n",
        "\n",
        "@dataclass\n",
        "class ProcessingConfig:\n",
        "    \"\"\"Configuration for audio processing\"\"\"\n",
        "    # Audio parameters\n",
        "    sample_rate: int = 44100\n",
        "    n_fft: int = 2048\n",
        "    hop_length: int = 512\n",
        "    pad_mode: str = 'constant'\n",
        "    \n",
        "    # Image processing\n",
        "    image_scale_factor: float = 1.0\n",
        "    image_chunk_size: int = 512\n",
        "    \n",
        "    # ControlNet parameters\n",
        "    enable_controlnet: bool = True\n",
        "    controlnet_strength: float = 0.75\n",
        "    guidance_scale: float = 7.5\n",
        "    num_inference_steps: int = 50\n",
        "    \n",
        "    # Optimization\n",
        "    torch_dtype: str = 'float16'\n",
        "    attention_slice_size: Optional[int] = 1\n",
        "    enable_xformers: bool = True\n",
        "    enable_cuda_graph: bool = False\n",
        "    \n",
        "    # Artifact processing\n",
        "    artifact_threshold_freq: float = 11000\n",
        "    artifact_smoothing_sigma: float = 1.0\n",
        "    temporal_smoothing: float = 2.0\n",
        "\n",
        "@dataclass\n",
        "class SeparationResult:\n",
        "    \"\"\"Data class for storing separation results\"\"\"\n",
        "    clean_vocal: AudioSegment\n",
        "    separated_vocal: AudioSegment\n",
        "    enhanced_vocal: Optional[AudioSegment]\n",
        "    accompaniment: AudioSegment\n",
        "    mixed: AudioSegment\n",
        "    file_paths: Dict[str, Path]\n",
        "    analysis_path: Optional[Path] = None\n",
        "    phase_analysis: Optional[Dict[str, Any]] = None\n",
        "    artifact_analysis: Optional[Dict[str, str]] = None\n",
        "\n",
        "# From enhancement\\controlnet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "class ArtifactDetector(nn.Module):\n",
        "    \"\"\"Preprocessor that generates artifact control maps\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Lightweight conv layers for artifact detection\n",
        "        self.detector = nn.Sequential(\n",
        "            nn.Conv2d(4, 16, 3, padding=1),  # 4 channels: RGB + phase\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 1, 1)  # Single channel artifact map\n",
        "        )\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: RGBA tensor where A channel contains phase information\n",
        "        Returns:\n",
        "            Single-channel artifact probability map\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.detector(x))\n",
        "\n",
        "class PhaseAwareZeroConv(nn.Module):\n",
        "    \"\"\"Zero convolution block that preserves phase information\"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_channels: int,\n",
        "        output_channels: int,\n",
        "        phase_channels: int = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.main_conv = nn.Conv2d(input_channels, output_channels, 1)\n",
        "        self.phase_conv = nn.Conv2d(phase_channels, phase_channels, 1)\n",
        "        \n",
        "        # Initialize to zero\n",
        "        nn.init.zeros_(self.main_conv.weight)\n",
        "        nn.init.zeros_(self.main_conv.bias)\n",
        "        nn.init.zeros_(self.phase_conv.weight)\n",
        "        nn.init.zeros_(self.phase_conv.bias)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, control: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Split main features and phase\n",
        "        features, phase = x[:, :-1], x[:, -1:]\n",
        "        \n",
        "        # Apply zero convolutions\n",
        "        out_features = self.main_conv(features * control)\n",
        "        out_phase = self.phase_conv(phase * control)\n",
        "        \n",
        "        return torch.cat([out_features, out_phase], dim=1)\n",
        "\n",
        "class PhaseAwareControlNet(nn.Module):\n",
        "    \"\"\"ControlNet adaptation for phase-aware spectrogram processing\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model: nn.Module,\n",
        "        control_channels: int = 1,\n",
        "        phase_channels: int = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.artifact_detector = ArtifactDetector()\n",
        "        \n",
        "        # Create zero conv layers for each injection point\n",
        "        self.zero_convs = nn.ModuleList([\n",
        "            PhaseAwareZeroConv(\n",
        "                base_model.get_feature_channels(i),\n",
        "                base_model.get_feature_channels(i),\n",
        "                phase_channels\n",
        "            )\n",
        "            for i in range(base_model.num_injection_points)\n",
        "        ])\n",
        "        \n",
        "    def forward(\n",
        "        self, \n",
        "        x: torch.Tensor,\n",
        "        timestep: Optional[torch.Tensor] = None,\n",
        "        context: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        # Generate control signal\n",
        "        control = self.artifact_detector(x)\n",
        "        \n",
        "        # Get intermediate features from frozen base model\n",
        "        features = self.base_model.get_feature_pyramid(x, timestep, context)\n",
        "        \n",
        "        # Apply controlled features through zero convs\n",
        "        controlled_features = []\n",
        "        for feat, zero_conv in zip(features, self.zero_convs):\n",
        "            controlled_features.append(zero_conv(feat, control))\n",
        "            \n",
        "        return self.base_model.forward_with_features(\n",
        "            x,\n",
        "            controlled_features,\n",
        "            timestep,\n",
        "            context\n",
        "        )\n",
        "\n",
        "# From enhancement\\training.py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ArtifactDataset(Dataset):\n",
        "    def __init__(self, \n",
        "                 clean_paths: List[str],\n",
        "                 separated_paths: List[str],\n",
        "                 preprocessor: HighFrequencyArtifactPreprocessor):\n",
        "        self.clean_paths = clean_paths\n",
        "        self.separated_paths = separated_paths\n",
        "        self.preprocessor = preprocessor\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.clean_paths)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        clean = load_audio(self.clean_paths[idx])\n",
        "        separated = load_audio(self.separated_paths[idx])\n",
        "        \n",
        "        condition, input_spec, target_spec = generate_training_pair(\n",
        "            clean, separated, self.preprocessor\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'condition': condition,\n",
        "            'input': input_spec,\n",
        "            'target': target_spec\n",
        "        }\n",
        "\n",
        "class ControlNetTrainer:\n",
        "    def __init__(self,\n",
        "                 model: PhaseAwareControlNet,\n",
        "                 preprocessor: HighFrequencyArtifactPreprocessor,\n",
        "                 learning_rate: float = 1e-4,\n",
        "                 device: str = 'cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.preprocessor = preprocessor.to(device)\n",
        "        self.device = device\n",
        "        \n",
        "        # Freeze base model, train only ControlNet components\n",
        "        for param in self.model.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "            lr=learning_rate\n",
        "        )\n",
        "        \n",
        "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # Move data to device\n",
        "        condition = batch['condition'].to(self.device)\n",
        "        input_spec = batch['input'].to(self.device)\n",
        "        target_spec = batch['target'].to(self.device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output = self.model(input_spec, control=condition)\n",
        "        \n",
        "        # Compute losses\n",
        "        losses = {\n",
        "            'magnitude': F.mse_loss(output[:, :-1], target_spec[:, :-1]),\n",
        "            'phase': F.mse_loss(output[:, -1:], target_spec[:, -1:]),\n",
        "            'frequency': self.frequency_loss(output, target_spec)\n",
        "        }\n",
        "        \n",
        "        # Combined loss\n",
        "        total_loss = sum(losses.values())\n",
        "        total_loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "            max_norm=1.0\n",
        "        )\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {k: v.item() for k, v in losses.items()}\n",
        "    \n",
        "    def frequency_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Frequency-domain loss focusing on high frequencies\"\"\"\n",
        "        # Convert to frequency domain\n",
        "        pred_fft = torch.fft.rfft2(pred[:, :-1])  # Exclude phase channel\n",
        "        target_fft = torch.fft.rfft2(target[:, :-1])\n",
        "        \n",
        "        # Weight higher frequencies more\n",
        "        freq_weights = torch.linspace(1.0, 2.0, pred_fft.size(-1))\n",
        "        freq_weights = freq_weights.to(self.device)\n",
        "        \n",
        "        return F.mse_loss(\n",
        "            pred_fft * freq_weights,\n",
        "            target_fft * freq_weights\n",
        "        )\n",
        "    \n",
        "    def train(self,\n",
        "             train_loader: DataLoader,\n",
        "             val_loader: Optional[DataLoader] = None,\n",
        "             epochs: int = 100,\n",
        "             save_dir: str = 'checkpoints'):\n",
        "        best_val_loss = float('inf')\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            train_losses = []\n",
        "            \n",
        "            for batch in train_loader:\n",
        "                losses = self.train_step(batch)\n",
        "                train_losses.append(losses)\n",
        "            \n",
        "            # Validation\n",
        "            if val_loader is not None:\n",
        "                val_loss = self.validate(val_loader)\n",
        "                \n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    self.save_checkpoint(f'{save_dir}/best.pt')\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                self.save_checkpoint(f'{save_dir}/epoch_{epoch}.pt')\n",
        "    \n",
        "    def validate(self, val_loader: DataLoader) -> float:\n",
        "        self.model.eval()\n",
        "        val_losses = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                condition = batch['condition'].to(self.device)\n",
        "                input_spec = batch['input'].to(self.device)\n",
        "                target_spec = batch['target'].to(self.device)\n",
        "                \n",
        "                output = self.model(input_spec, control=condition)\n",
        "                loss = F.mse_loss(output, target_spec)\n",
        "                val_losses.append(loss.item())\n",
        "        \n",
        "        return sum(val_losses) / len(val_losses)\n",
        "    \n",
        "    def save_checkpoint(self, path: str):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict()\n",
        "        }, path)\n",
        "    \n",
        "    def load_checkpoint(self, path: str):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "def prepare_training(\n",
        "    clean_dir: str,\n",
        "    separated_dir: str,\n",
        "    batch_size: int = 8,\n",
        "    val_split: float = 0.1\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    # Get audio paths\n",
        "    clean_paths = sorted(glob.glob(f'{clean_dir}/*.wav'))\n",
        "    separated_paths = sorted(glob.glob(f'{separated_dir}/*.wav'))\n",
        "    \n",
        "    # Split train/val\n",
        "    split_idx = int(len(clean_paths) * (1 - val_split))\n",
        "    \n",
        "    # Create datasets\n",
        "    preprocessor = HighFrequencyArtifactPreprocessor()\n",
        "    train_dataset = ArtifactDataset(\n",
        "        clean_paths[:split_idx],\n",
        "        separated_paths[:split_idx],\n",
        "        preprocessor\n",
        "    )\n",
        "    val_dataset = ArtifactDataset(\n",
        "        clean_paths[split_idx:],\n",
        "        separated_paths[split_idx:],\n",
        "        preprocessor\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "# From separation\\base.py\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum, auto\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class SeparationProfile:\n",
        "    backend: SeparatorBackend\n",
        "    preserve_high_freq: bool = False\n",
        "    target_sample_rate: int = 44100\n",
        "    min_segment_length: float = 5.0\n",
        "    # Added enhancement parameters\n",
        "    use_phase_aware_controlnet: bool = False\n",
        "    use_high_freq_processor: bool = True\n",
        "    artifact_reduction_config: Optional[ProcessingConfig] = None\n",
        "    controlnet_model_path: Optional[Path] = None\n",
        "\n",
        "@dataclass\n",
        "class SeparationResult:\n",
        "    clean_vocal: AudioSegment\n",
        "    separated_vocal: AudioSegment\n",
        "    enhanced_vocal: Optional[AudioSegment]  # After artifact reduction\n",
        "    accompaniment: AudioSegment\n",
        "    mixed: AudioSegment\n",
        "    analysis_path: Optional[Path] = None\n",
        "    phase_analysis: Optional[Dict[str, Any]] = None\n",
        "    artifact_analysis: Optional[Dict[str, str]] = None\n",
        "\n",
        "class StemProcessor:\n",
        "    def __init__(self, profile: SeparationProfile):\n",
        "        self.profile = profile\n",
        "        self.separator = self._create_separator()\n",
        "        self.analyzer = VocalSeparationAnalyzer(Path(\"analysis_output\"))\n",
        "        \n",
        "        # Initialize enhancement components\n",
        "        self.artifact_processor = IntegratedHighFrequencyProcessor(\n",
        "            \"enhancement_output\",\n",
        "            config=profile.artifact_reduction_config or ProcessingConfig()\n",
        "        ) if profile.use_high_freq_processor else None\n",
        "        \n",
        "        self.controlnet = PhaseAwareControlNet.from_pretrained(\n",
        "            profile.controlnet_model_path\n",
        "        ) if profile.use_phase_aware_controlnet else None\n",
        "\n",
        "    def process_stems(self, \n",
        "                     vocal_paths: Tuple[str, str],\n",
        "                     accompaniment_paths: Tuple[str, str],\n",
        "                     start_time: float = 0.0,\n",
        "                     duration: float = 30.0) -> SeparationResult:\n",
        "        try:\n",
        "            self.separator.setup()\n",
        "            \n",
        "            # Load and prepare audio\n",
        "            vocals = self._load_stereo_pair(*vocal_paths, start_time, duration)\n",
        "            accompaniment = self._load_stereo_pair(*accompaniment_paths, start_time, duration)\n",
        "            mixed = AudioSegment(\n",
        "                audio=vocals.audio + accompaniment.audio,\n",
        "                sample_rate=vocals.sample_rate,\n",
        "                start_time=start_time,\n",
        "                duration=duration\n",
        "            )\n",
        "\n",
        "            # Initial separation\n",
        "            separated = self.separator.separate(mixed)\n",
        "            base_analysis = self.analyzer.analyze(vocals, separated)\n",
        "            \n",
        "            # Enhancement pipeline\n",
        "            enhanced = separated\n",
        "            artifact_analysis = None\n",
        "            \n",
        "            # Apply high-frequency artifact reduction if enabled\n",
        "            if self.artifact_processor:\n",
        "                enhanced_audio, analysis_files = self.artifact_processor.process_and_analyze(\n",
        "                    enhanced.audio\n",
        "                )\n",
        "                enhanced = AudioSegment(\n",
        "                    audio=enhanced_audio,\n",
        "                    sample_rate=enhanced.sample_rate,\n",
        "                    start_time=enhanced.start_time,\n",
        "                    duration=enhanced.duration\n",
        "                )\n",
        "                artifact_analysis = analysis_files\n",
        "            \n",
        "            # Apply ControlNet enhancement if enabled\n",
        "            if self.controlnet:\n",
        "                enhanced = self._apply_controlnet_enhancement(enhanced)\n",
        "            \n",
        "            return SeparationResult(\n",
        "                clean_vocal=vocals,\n",
        "                separated_vocal=separated,\n",
        "                enhanced_vocal=enhanced,\n",
        "                accompaniment=accompaniment,\n",
        "                mixed=mixed,\n",
        "                analysis_path=base_analysis,\n",
        "                artifact_analysis=artifact_analysis\n",
        "            )\n",
        "        finally:\n",
        "            self.separator.cleanup()\n",
        "\n",
        "    def _apply_controlnet_enhancement(self, audio: AudioSegment) -> AudioSegment:\n",
        "        # Convert to spectrogram\n",
        "        spec = self.artifact_processor.create_complex_spectrogram(\n",
        "            audio.audio\n",
        "        ) if self.artifact_processor else None\n",
        "        \n",
        "        # Process through ControlNet\n",
        "        enhanced_spec = self.controlnet(spec)\n",
        "        \n",
        "        # Convert back to audio\n",
        "        enhanced_audio = self.artifact_processor.spectrogram_to_audio(\n",
        "            enhanced_spec\n",
        "        ) if self.artifact_processor else None\n",
        "        \n",
        "        return AudioSegment(\n",
        "            audio=enhanced_audio,\n",
        "            sample_rate=audio.sample_rate,\n",
        "            start_time=audio.start_time,\n",
        "            duration=audio.duration\n",
        "        )\n",
        "\n",
        "# From analysis\\base.py\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "# from ..core.audio import AudioSegment  # Original relative import\n",
        "\n",
        "class VocalAnalyzer(ABC):\n",
        "    \"\"\"Base class for vocal analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: Path):\n",
        "        self.output_dir = output_dir\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    @abstractmethod\n",
        "    def analyze(self, clean: AudioSegment, separated: AudioSegment) -> Path:\n",
        "        \"\"\"Perform analysis and return path to analysis results\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _create_spectrograms(self, clean: np.ndarray, separated: np.ndarray, \n",
        "                          sr: int, output_path: Path):\n",
        "        \"\"\"Create and save spectrogram comparisons\"\"\"\n",
        "        pass\n",
        "\n",
        "# From analysis\\artifacts\\spectral.py\n",
        "import librosa\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n",
        "# from ...core.audio import AudioSegment  # Original relative import\n",
        "\n",
        "\n",
        "class SpectralAnalyzer:\n",
        "    \"\"\"Spectral analysis and visualization\"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: Path, config: Optional[ProcessingConfig] = None):\n",
        "        self.output_dir = output_dir\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.config = config or ProcessingConfig()\n",
        "        self.normalization_params = {}\n",
        "        \n",
        "    def _save_comparison(self, \n",
        "                        clean_spec: np.ndarray, \n",
        "                        separated_spec: np.ndarray, \n",
        "                        path: Path):\n",
        "        \"\"\"Save visual comparison of spectrograms\"\"\"\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 16))\n",
        "        \n",
        "        # Plot clean spectrogram\n",
        "        self._plot_spectrogram(\n",
        "            clean_spec, \n",
        "            ax1, \n",
        "            \"Clean Vocal Spectrogram\",\n",
        "            self.config.sample_rate\n",
        "        )\n",
        "        \n",
        "        # Plot separated spectrogram\n",
        "        self._plot_spectrogram(\n",
        "            separated_spec, \n",
        "            ax2, \n",
        "            \"Separated Vocal Spectrogram\",\n",
        "            self.config.sample_rate\n",
        "        )\n",
        "        \n",
        "        # Plot difference\n",
        "        difference = librosa.amplitude_to_db(\n",
        "            np.abs(separated_spec) - np.abs(clean_spec),\n",
        "            ref=np.max\n",
        "        )\n",
        "        self._plot_spectrogram(\n",
        "            difference,\n",
        "            ax3,\n",
        "            \"Difference (Separated - Clean)\",\n",
        "            self.config.sample_rate,\n",
        "            cmap='RdBu'\n",
        "        )\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(path)\n",
        "        plt.close()\n",
        "        \n",
        "    def _plot_spectrogram(self,\n",
        "                         spec: np.ndarray,\n",
        "                         ax: plt.Axes,\n",
        "                         title: str,\n",
        "                         sr: int,\n",
        "                         cmap: str = 'magma'):\n",
        "        \"\"\"Helper to plot a single spectrogram\"\"\"\n",
        "        librosa.display.specshow(\n",
        "            librosa.amplitude_to_db(np.abs(spec), ref=np.max),\n",
        "            y_axis='mel',\n",
        "            x_axis='time',\n",
        "            sr=sr,\n",
        "            hop_length=self.config.hop_length,\n",
        "            ax=ax,\n",
        "            cmap=cmap\n",
        "        )\n",
        "        ax.set_title(title, color='white')\n",
        "        ax.set_xlabel('Time (s)', color='white')\n",
        "        ax.set_ylabel('Frequency (Hz)', color='white')\n",
        "        ax.tick_params(colors='white')\n",
        "        plt.colorbar(ax.collections[0], ax=ax, format='%+2.0f dB')\n",
        "    \n",
        "    def _analyze_differences(self, \n",
        "                           clean_spec: np.ndarray, \n",
        "                           separated_spec: np.ndarray) -> Dict:\n",
        "        \"\"\"Analyze spectral differences between clean and separated audio\"\"\"\n",
        "        # Get magnitude spectrograms\n",
        "        clean_mag = np.abs(clean_spec)\n",
        "        sep_mag = np.abs(separated_spec)\n",
        "        \n",
        "        # Get phase information\n",
        "        clean_phase = np.angle(clean_spec)\n",
        "        sep_phase = np.angle(separated_spec)\n",
        "        \n",
        "        # Calculate frequency bands\n",
        "        freq_bins = librosa.fft_frequencies(\n",
        "            sr=self.config.sample_rate,\n",
        "            n_fft=self.config.n_fft\n",
        "        )\n",
        "        \n",
        "        bands = {\n",
        "            \"sub_bass\": (20, 60),\n",
        "            \"bass\": (60, 250),\n",
        "            \"low_mid\": (250, 500),\n",
        "            \"mid\": (500, 2000),\n",
        "            \"high_mid\": (2000, 4000),\n",
        "            \"presence\": (4000, 6000),\n",
        "            \"brilliance\": (6000, 20000)\n",
        "        }\n",
        "        \n",
        "        analysis = {}\n",
        "        \n",
        "        # Analyze each frequency band\n",
        "        for band_name, (low_freq, high_freq) in bands.items():\n",
        "            # Get band indices\n",
        "            band_mask = (freq_bins >= low_freq) & (freq_bins <= high_freq)\n",
        "            \n",
        "            # Magnitude difference in band\n",
        "            mag_diff = np.mean(np.abs(sep_mag[band_mask] - clean_mag[band_mask]))\n",
        "            \n",
        "            # Phase coherence in band\n",
        "            phase_diff = np.abs(sep_phase[band_mask] - clean_phase[band_mask])\n",
        "            phase_coherence = np.mean(np.cos(phase_diff))\n",
        "            \n",
        "            # RMS energy ratio\n",
        "            clean_rms = np.sqrt(np.mean(clean_mag[band_mask] ** 2))\n",
        "            sep_rms = np.sqrt(np.mean(sep_mag[band_mask] ** 2))\n",
        "            energy_ratio = sep_rms / clean_rms if clean_rms > 0 else 0\n",
        "            \n",
        "            analysis[band_name] = {\n",
        "                \"magnitude_difference\": float(mag_diff),\n",
        "                \"phase_coherence\": float(phase_coherence),\n",
        "                \"energy_ratio\": float(energy_ratio)\n",
        "            }\n",
        "        \n",
        "        # Overall statistics\n",
        "        analysis[\"overall\"] = {\n",
        "            \"total_magnitude_difference\": float(np.mean(np.abs(sep_mag - clean_mag))),\n",
        "            \"average_phase_coherence\": float(np.mean(np.cos(np.abs(sep_phase - clean_phase)))),\n",
        "            \"total_energy_ratio\": float(np.sum(sep_mag) / np.sum(clean_mag))\n",
        "        }\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def _save_analysis(self, analysis: Dict, path: Path):\n",
        "        \"\"\"Save analysis results as JSON\"\"\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(analysis, f, indent=2)\n",
        "\n",
        "# From io\\audio.py\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from typing import Tuple\n",
        "# from ..core.audio import AudioSegment  # Original relative import\n",
        "\n",
        "def load_audio_file(path: str, sr: int = 44100, mono: bool = False) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load audio file with error handling and validation\"\"\"\n",
        "    try:\n",
        "        audio, file_sr = librosa.load(path, sr=sr, mono=mono)\n",
        "        return audio, file_sr\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading audio file {path}: {str(e)}\")\n",
        "\n",
        "def save_audio_file(audio: AudioSegment, path: Path) -> None:\n",
        "    \"\"\"Save audio file with proper format handling\"\"\"\n",
        "    try:\n",
        "        # Handle different array shapes\n",
        "        audio_to_save = audio.audio\n",
        "        if audio.is_stereo:\n",
        "            # Convert from (2, samples) to (samples, 2)\n",
        "            audio_to_save = audio.audio.T\n",
        "            \n",
        "        sf.write(str(path), audio_to_save, audio.sample_rate)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error saving audio file {path}: {str(e)}\")\n",
        "\n",
        "# From enhancement\\base.py\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional\n",
        "# from ...core.audio import AudioSegment  # Original relative import\n",
        "# from ...core.types import ProcessingConfig  # Original relative import\n",
        "\n",
        "class EnhancementProcessor(ABC):\n",
        "    \"\"\"Base class for audio enhancement processors\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Optional[ProcessingConfig] = None):\n",
        "        self.config = config or ProcessingConfig()\n",
        "    \n",
        "    @abstractmethod\n",
        "    def enhance(self, audio: AudioSegment) -> AudioSegment:\n",
        "        \"\"\"Enhance audio segment\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def validate(self, audio: AudioSegment) -> dict:\n",
        "        \"\"\"Validate enhancement results\"\"\"\n",
        "        pass\n",
        "\n",
        "# From preparation\\base.py\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict\n",
        "# from ..core.types import SeparationResult  # Original relative import\n",
        "# from ..core.audio import AudioSegment  # Original relative import\n",
        "\n",
        "class VocalSeparator(ABC):\n",
        "    \"\"\"Abstract base class for vocal separators\"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: str = \"output\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    @abstractmethod\n",
        "    def separate_and_analyze(self,\n",
        "                           vocal_paths: Tuple[str, str],\n",
        "                           accompaniment_paths: Tuple[str, str],\n",
        "                           start_time: float = 0.0,\n",
        "                           duration: float = 30.0,\n",
        "                           run_analysis: bool = True) -> SeparationResult:\n",
        "        \"\"\"Perform separation and analysis\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _load_stereo_pair(self, left_path: str, right_path: str, \n",
        "                         start_time: float, duration: float) -> AudioSegment:\n",
        "        \"\"\"Load and process stereo pair\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _separate_vocals(self, mixed: AudioSegment) -> AudioSegment:\n",
        "        \"\"\"Perform vocal separation\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _save_audio_files(self, vocals: AudioSegment, \n",
        "                         accompaniment: AudioSegment,\n",
        "                         mixed: AudioSegment, \n",
        "                         separated: AudioSegment,\n",
        "                         start_time: float) -> Dict[str, Path]:\n",
        "        \"\"\"Save all audio files\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources - override if needed\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "\n",
        "# From separation\\spleeter.py\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "from spleeter.separator import Separator as SpleeterBase\n",
        "\n",
        "# from .base import VocalSeparator  # Original relative import\n",
        "# from ..core.audio import AudioSegment  # Original relative import\n",
        "# from ..core.types import SeparationResult  # Original relative import\n",
        "# from ..io.audio import load_audio_file, save_audio_file  # Original relative import\n",
        "# from ..analysis.spectral import SpectralAnalyzer  # Original relative import\n",
        "\n",
        "class SpleeterSeparator(VocalSeparator):\n",
        "    \"\"\"Concrete implementation using Spleeter\"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: str = \"output\"):\n",
        "        super().__init__(output_dir)\n",
        "        \n",
        "        # Initialize components\n",
        "        self.analyzer = SpectralAnalyzer(self.output_dir)\n",
        "        \n",
        "        # Defer TensorFlow setup until needed\n",
        "        self.separator = None\n",
        "        self.graph = None\n",
        "        self.session = None\n",
        "\n",
        "    def _setup_tensorflow(self):\n",
        "        \"\"\"Setup TensorFlow session and graph - called only when needed\"\"\"\n",
        "        if self.separator is not None:\n",
        "            return  # Already initialized\n",
        "            \n",
        "        # Create a new graph and session without resetting\n",
        "        self.graph = tf.Graph()\n",
        "        self.graph.as_default().__enter__()\n",
        "        \n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.session = tf.compat.v1.Session(graph=self.graph, config=config)\n",
        "        self.session.as_default().__enter__()\n",
        "        \n",
        "        # Initialize Spleeter only after graph/session setup\n",
        "        self.separator = SpleeterBase('spleeter:2stems')\n",
        "\n",
        "    def separate_and_analyze(self,\n",
        "                           vocal_paths: Tuple[str, str],\n",
        "                           accompaniment_paths: Tuple[str, str],\n",
        "                           start_time: float = 0.0,\n",
        "                           duration: float = 30.0,\n",
        "                           run_analysis: bool = True) -> SeparationResult:\n",
        "        \"\"\"Main method to perform separation and analysis\"\"\"\n",
        "        # Ensure TensorFlow is set up\n",
        "        self._setup_tensorflow()\n",
        "        \n",
        "        # Load audio\n",
        "        vocals = self._load_stereo_pair(*vocal_paths, start_time, duration)\n",
        "        accompaniment = self._load_stereo_pair(*accompaniment_paths, start_time, duration)\n",
        "\n",
        "        # Create mix\n",
        "        mixed = AudioSegment(\n",
        "            audio=vocals.audio + accompaniment.audio,\n",
        "            sample_rate=vocals.sample_rate,\n",
        "            start_time=start_time,\n",
        "            duration=duration\n",
        "        )\n",
        "\n",
        "        # Perform separation\n",
        "        separated = self._separate_vocals(mixed)\n",
        "        \n",
        "        # Save files\n",
        "        file_paths = self._save_audio_files(\n",
        "            vocals, accompaniment, mixed, separated, start_time\n",
        "        )\n",
        "\n",
        "        result = SeparationResult(\n",
        "            clean_vocal=vocals,\n",
        "            separated_vocal=separated,\n",
        "            accompaniment=accompaniment,\n",
        "            mixed=mixed,\n",
        "            file_paths=file_paths\n",
        "        )\n",
        "\n",
        "        # Run analysis if requested\n",
        "        if run_analysis:\n",
        "            result.analysis_path = self.analyzer.analyze(vocals, separated)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _load_stereo_pair(self, left_path: str, right_path: str, \n",
        "                         start_time: float, duration: float) -> AudioSegment:\n",
        "        \"\"\"Load and process stereo pair\"\"\"\n",
        "        print(f\"Loading {left_path}...\")\n",
        "        left, sr = load_audio_file(left_path, sr=44100, mono=True)\n",
        "        print(f\"Left channel length: {len(left)} samples ({len(left)/44100:.2f} seconds)\")\n",
        "\n",
        "        print(f\"Loading {right_path}...\")\n",
        "        right, _ = load_audio_file(right_path, sr=44100, mono=True)\n",
        "        print(f\"Right channel length: {len(right)} samples ({len(right)/44100:.2f} seconds)\")\n",
        "\n",
        "        # Ensure same length\n",
        "        min_length = min(len(left), len(right))\n",
        "        left = left[:min_length]\n",
        "        right = right[:min_length]\n",
        "        print(f\"Adjusted stereo length: {min_length} samples ({min_length/44100:.2f} seconds)\")\n",
        "\n",
        "        # Extract segment\n",
        "        start_sample = int(start_time * 44100)\n",
        "        duration_samples = int(duration * 44100)\n",
        "        \n",
        "        if start_sample + duration_samples > min_length:\n",
        "            print(f\"Warning: Requested duration extends beyond audio length. Truncating.\")\n",
        "            duration_samples = min_length - start_sample\n",
        "        \n",
        "        left_segment = left[start_sample:start_sample + duration_samples]\n",
        "        right_segment = right[start_sample:start_sample + duration_samples]\n",
        "\n",
        "        # Stack to stereo\n",
        "        stereo = np.vstack([left_segment, right_segment])\n",
        "        \n",
        "        return AudioSegment(\n",
        "            audio=stereo,\n",
        "            sample_rate=44100,\n",
        "            start_time=start_time,\n",
        "            duration=duration_samples/44100\n",
        "        )\n",
        "\n",
        "    def _separate_vocals(self, mixed: AudioSegment) -> AudioSegment:\n",
        "        \"\"\"Perform vocal separation\"\"\"\n",
        "        # Convert to mono and reshape for Spleeter\n",
        "        mix_mono = mixed.to_mono().audio\n",
        "        mix_mono = mix_mono.reshape(-1, 1)\n",
        "\n",
        "        print(f\"Mix shape before separation: {mix_mono.shape}\")\n",
        "        print(\"Running separation...\")\n",
        "        \n",
        "        separated = self.separator.separate(mix_mono)\n",
        "        separated_vocals = separated['vocals']\n",
        "        print(f\"Separated vocals shape: {separated_vocals.shape}\")\n",
        "        \n",
        "        # Since Spleeter returns (samples, channels), we should handle it accordingly\n",
        "        if len(separated_vocals.shape) == 2:\n",
        "            if separated_vocals.shape[1] == 2:\n",
        "                # If it's stereo, convert to our preferred format (2, samples)\n",
        "                separated_vocals = separated_vocals.T\n",
        "            elif separated_vocals.shape[1] == 1:\n",
        "                # If it's mono in (samples, 1) shape, convert to 1D array\n",
        "                separated_vocals = separated_vocals.reshape(-1)\n",
        "                \n",
        "        print(f\"Final separated vocals shape: {separated_vocals.shape}\")\n",
        "        \n",
        "        if len(separated_vocals.shape) == 1:\n",
        "            # If mono, duplicate to stereo to match input\n",
        "            separated_vocals = np.vstack([separated_vocals, separated_vocals])\n",
        "        \n",
        "        print(f\"Output separated vocals shape: {separated_vocals.shape}\")\n",
        "            \n",
        "        if separated_vocals.shape[1] < 1000:  # Sanity check on the correct dimension\n",
        "            raise ValueError(f\"Separated vocals seem too short: {separated_vocals.shape}\")\n",
        "                \n",
        "        return AudioSegment(\n",
        "            audio=separated_vocals,\n",
        "            sample_rate=mixed.sample_rate,\n",
        "            start_time=mixed.start_time,\n",
        "            duration=mixed.duration\n",
        "        )\n",
        "\n",
        "    def _save_audio_files(self, vocals: AudioSegment, accompaniment: AudioSegment,\n",
        "                         mixed: AudioSegment, separated: AudioSegment,\n",
        "                         start_time: float) -> Dict[str, Path]:\n",
        "        \"\"\"Save all audio files\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_dir = self.output_dir / f\"separation_{timestamp}\"\n",
        "        save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        files = {}\n",
        "        \n",
        "        # Prepare audio segments for saving\n",
        "        segments = {\n",
        "            'clean_vocal': vocals,\n",
        "            'accompaniment': accompaniment,\n",
        "            'mix': mixed,\n",
        "            'separated_vocal': separated\n",
        "        }\n",
        "\n",
        "        for name, segment in segments.items():\n",
        "            path = save_dir / f\"{name}_from_{start_time:.1f}s.wav\"\n",
        "            print(f\"\\nProcessing {name}:\")\n",
        "            print(f\"Original shape: {segment.audio.shape}\")\n",
        "            \n",
        "            save_audio_file(segment, path)\n",
        "            files[name] = path\n",
        "\n",
        "        return files\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        try:\n",
        "            if hasattr(self, 'separator') and hasattr(self.separator, '_get_model'):\n",
        "                self.separator._get_model.cache_clear()\n",
        "            \n",
        "            if hasattr(self, 'session'):\n",
        "                self.session.as_default().__exit__(None, None, None)\n",
        "                self.session.close()\n",
        "                delattr(self, 'session')\n",
        "            \n",
        "            if hasattr(self, 'graph'):\n",
        "                self.graph.as_default().__exit__(None, None, None)\n",
        "                delattr(self, 'graph')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Warning during cleanup: {str(e)}\")\n",
        "\n",
        "    @property\n",
        "    def capabilities(self) -> Dict[str, any]:\n",
        "        \"\"\"Report capabilities/limitations of this backend\"\"\"\n",
        "        return {\n",
        "            \"max_frequency\": 11000,  # Hz\n",
        "            \"supports_stereo\": True,\n",
        "            \"native_sample_rate\": 22050,\n",
        "            \"recommended_min_segment\": 5.0  # seconds\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        " /\\___/\\   ChimeraCat\n",
        "( o   o )  Modular Python Fusion\n",
        "(  =^=  )  https://github.com/scottvr/chimeracat\n",
        " (______)  Generated: 2024-11-20 04:29:43\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}