
##Notebook Generated by ChimeraCat

# Generated by ChimeraCat
#  /\___/\  ChimeraCat
# ( o   o )  Modular Python Fusion
# (  =^=  )
#  (______)  Generated: 2024-11-27 00:38:49
        # Compression Level: verbose
        
"""

Directory Structure:
stemprover\__init__.py
stemprover\analysis\base.py
stemprover\analysis\artifacts\base.py
stemprover\analysis\artifacts\high_freq.py
stemprover\analysis\artifacts\preprocessor.py
stemprover\analysis\artifacts\spectral.py
stemprover\analysis\selection\metrics.py
stemprover\analysis\selection\segment_finder.py
stemprover\analysis\selection\__init__.py
stemprover\common\audio_utils.py
stemprover\common\math_utils.py
stemprover\common\spectral_utils.py
stemprover\common\types.py
stemprover\common\__init__.py
stemprover\core\audio.py
stemprover\core\config.py
stemprover\core\types.py
stemprover\enhancement\base.py
stemprover\enhancement\controlnet.py
stemprover\enhancement\training.py
stemprover\io\audio.py
stemprover\preparation\base.py
stemprover\preparation\segments\generator.py
stemprover\preparation\segments\__init__.py
stemprover\separation\base.py
stemprover\separation\spleeter.py
stemprover\training\dataset.py
stemprover\training\pairs.py
tools\chimeracat.py
    
Module Dependencies:
graph-easy not found. Install with: cpan Graph::Easy
    
Import Summary:

    External Dependencies:
    abc, common.audio_utils, common.spectral_utils, common.types, core.audio, core.types, dataclasses, datetime, enum, json, librosa, matplotlib.pyplot as plt, numpy as np, pathlib, soundfile as sf, spleeter.separator, tensorflow as tf, torch, torch.nn as nn, torch.nn.functional as F, torch.utils.data, typing
    
    Internal Dependencies:
    ...common.audio_utils, ...common.math_utils, ...common.types, ...core.audio, ...core.types, ..analysis.spectral, ..core.audio, ..core.types, ..io.audio, .analysis.base, .analysis.phase, .analysis.spectral, .audio, .audio_utils, .base, .core.audio, .core.types, .diffusion.models, .diffusion.training, .math_utils, .metrics, .preparation.segments, .separation.base, .separation.spleeter, .spectral_utils, .types
    
    
# External imports
"""
import abc
import common.audio_utils
import common.spectral_utils
import common.types
import core.audio
import core.types
import dataclasses
import datetime
import enum
import json
import librosa
import matplotlib.pyplot as plt
import numpy as np
import pathlib
import soundfile as sf
import spleeter.separator
import tensorflow as tf
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data
import typing

# Combined module code


# From stemprover\__init__.py
"""Stemprover - audio stem separation enhancement tools"""

try:
    """from ._version import version as __version__  # Original relative import"""
except ImportError:
    __version__ = "unknown"

# Core imports
"""from .core.audio import AudioSegment  # Original relative import"""
"""from .core.types import (  # Original relative import"""
    ProcessingConfig,
    SeparationResult
)

# Separation components
"""from .separation.base import VocalSeparator  # Original relative import"""
"""from .separation.spleeter import SpleeterSeparator  # Original relative import"""

# Analysis components
"""from .analysis.base import VocalAnalyzer  # Original relative import"""
"""from .analysis.spectral import SpectralAnalyzer  # Original relative import"""
"""from .analysis.phase import PhaseAnalyzer  # Original relative import"""

# Future diffusion components
"""from .diffusion.models import PhaseAwareLoRA  # Original relative import"""
"""from .diffusion.training import PhaseAwareTrainer  # Original relative import"""

__all__ = [
    # Version
    '__version__',
    
    # Core
    'AudioSegment',
    'ProcessingConfig',
    'SeparationResult',
    
    # Separation
    'VocalSeparator',
    'SpleeterSeparator',
    
    # Analysis
    'VocalAnalyzer',
    'SpectralAnalyzer',
    'PhaseAnalyzer',
    
    # Diffusion
    'PhaseAwareLoRA',
    'PhaseAwareTrainer',
]

# From stemprover\analysis\base.py
from abc import ABC, abstractmethod
from pathlib import Path
"""from ..core.audio import AudioSegment  # Original relative import"""

class VocalAnalyzer(ABC):
    """Base class for vocal analysis"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def analyze(self, clean: AudioSegment, separated: AudioSegment) -> Path:
        """Perform analysis and return path to analysis results"""
        pass

    @abstractmethod
    def _create_spectrograms(self, clean: np.ndarray, separated: np.ndarray, 
                          sr: int, output_path: Path):
        """Create and save spectrogram comparisons"""
        pass

# From stemprover\analysis\artifacts\base.py
"""from ...common.types import AudioArray, SpectrogramArray, TensorType  # Original relative import"""
"""from ...common.audio_utils import create_spectrogram  # Original relative import"""

from abc import ABC, abstractmethod
import numpy as np
import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from pathlib import Path

@dataclass
class ValidationMetrics:
    """Stores comparison metrics between original and processed audio"""
    phase_coherence: float
    frequency_response: Dict[str, float]
    signal_to_noise: float
    processing_time: float
    memory_usage: float
    
    def as_dict(self) -> Dict[str, float]:
        return {
            "phase_coherence": self.phase_coherence,
            "freq_response_mean": np.mean(list(self.frequency_response.values())),
            "freq_response_std": np.std(list(self.frequency_response.values())),
            "snr": self.signal_to_noise,
            "processing_time": self.processing_time,
            "memory_usage": self.memory_usage
        }

class ArtifactProcessor(ABC):
    """Base class for different artifact processing approaches"""
    
    def __init__(self, sample_rate: int = 44100):
        self.sample_rate = sample_rate
        self.frequency_bands = {
            "sub_bass": (20, 60),
            "bass": (60, 250),
            "low_mid": (250, 500),
            "mid": (500, 2000),
            "high_mid": (2000, 4000),
            "presence": (4000, 6000),
            "brilliance": (6000, 20000)
        }
    
    @abstractmethod
    def process(self, 
                audio_segment: AudioSegment,
                artifact_type: str) -> AudioSegment:
        """Process audio to remove specified artifact type"""
        pass
    
    def validate(self, 
                clean: AudioSegment,
                processed: AudioSegment,
                original_artifacts: AudioSegment) -> ValidationMetrics:
        """Calculate validation metrics comparing processed output to clean reference"""
        
        # Calculate phase coherence
        phase_coherence = self._measure_phase_coherence(
            clean.audio, processed.audio
        )
        
        # Analyze frequency response
        freq_response = self._analyze_frequency_response(
            clean.audio, processed.audio
        )
        
        # Calculate SNR using original artifacts as noise reference
        snr = self._calculate_snr(
            clean.audio,
            processed.audio,
            original_artifacts.audio
        )
        
        return ValidationMetrics(
            phase_coherence=phase_coherence,
            frequency_response=freq_response,
            signal_to_noise=snr,
            processing_time=0.0,  # Set by wrapper
            memory_usage=0.0  # Set by wrapper
        )
    
    def _measure_phase_coherence(self,
                               clean: np.ndarray,
                               processed: np.ndarray) -> float:
        """Measure phase coherence between clean and processed audio"""
        # Use STFT for phase analysis
        n_fft = 2048
        hop_length = 512
        
        clean_stft = librosa.stft(clean, n_fft=n_fft, hop_length=hop_length)
        proc_stft = librosa.stft(processed, n_fft=n_fft, hop_length=hop_length)
        
        # Calculate phase difference
        clean_phase = np.angle(clean_stft)
        proc_phase = np.angle(proc_stft)
        phase_diff = np.abs(clean_phase - proc_phase)
        
        # Return mean phase coherence (1 = perfect, 0 = random)
        return float(np.mean(np.cos(phase_diff)))
    
    def _analyze_frequency_response(self,
                                  clean: np.ndarray,
                                  processed: np.ndarray) -> Dict[str, float]:
        """Analyze frequency response preservation in different bands"""
        results = {}
        
        for band_name, (low, high) in self.frequency_bands.items():
            # Filter both signals to band
            clean_band = self._bandpass_filter(clean, low, high)
            proc_band = self._bandpass_filter(processed, low, high)
            
            # Calculate RMS difference
            clean_rms = np.sqrt(np.mean(clean_band ** 2))
            proc_rms = np.sqrt(np.mean(proc_band ** 2))
            
            # Store ratio (1 = perfect preservation)
            results[band_name] = proc_rms / clean_rms if clean_rms > 0 else 0.0
            
        return results
    
    def _bandpass_filter(self,
                        audio: np.ndarray,
                        low_freq: float,
                        high_freq: float) -> np.ndarray:
        """Apply bandpass filter to audio"""
        nyquist = self.sample_rate // 2
        low = low_freq / nyquist
        high = high_freq / nyquist
        
        # Design filter
        b, a = scipy.signal.butter(4, [low, high], btype='band')
        
        # Apply filter
        return scipy.signal.filtfilt(b, a, audio)
    
    def _calculate_snr(self,
                      clean: np.ndarray,
                      processed: np.ndarray,
                      noise: np.ndarray) -> float:
        """Calculate signal-to-noise ratio"""
        signal_power = np.mean(clean ** 2)
        noise_power = np.mean((processed - clean) ** 2)
        
        return 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')

class ControlNetProcessor(ArtifactProcessor):
    """ControlNet-based artifact processor"""
    
    def __init__(self, model_path: Optional[Path] = None):
        super().__init__()
        self.model = PhaseAwareControlNet(...)  # Initialize from earlier code
        if model_path:
            self.model.load_state_dict(torch.load(model_path))
    
    def process(self,
                audio_segment: AudioSegment,
                artifact_type: str) -> AudioSegment:
        # Convert to spectrogram
        spec = self._audio_to_spectrogram(audio_segment)
        
        # Process through ControlNet
        processed_spec = self.model(spec)
        
        # Convert back to audio
        return self._spectrogram_to_audio(processed_spec)

class SignalProcessor(ArtifactProcessor):
    """Direct signal-domain processor"""
    
    def __init__(self):
        super().__init__()
        
    def process(self,
                audio_segment: AudioSegment,
                artifact_type: str) -> AudioSegment:
        # Implement direct signal processing approach
        # (We'll flesh this out as we identify specific artifacts)
        pass

class HybridProcessor(ArtifactProcessor):
    """Hybrid approach combining latent and signal processing"""
    
    def __init__(self):
        super().__init__()
        self.controlnet = ControlNetProcessor()
        self.signal = SignalProcessor()
        
    def process(self,
                audio_segment: AudioSegment,
                artifact_type: str) -> AudioSegment:
        # Route to appropriate processor based on artifact type
        # Or combine both approaches
        pass

def run_validation(
    processor: ArtifactProcessor,
    test_cases: List[Tuple[AudioSegment, AudioSegment, AudioSegment]],
    artifact_types: List[str]
) -> Dict[str, List[ValidationMetrics]]:
    """Run validation suite on processor"""
    
    results = {artifact_type: [] for artifact_type in artifact_types}
    
    for clean, artifacts, mixed in test_cases:
        for artifact_type in artifact_types:
            # Time and memory measurement wrapper
            start_time = time.time()
            start_mem = psutil.Process().memory_info().rss
            
            # Process audio
            processed = processor.process(mixed, artifact_type)
            
            # Get metrics
            metrics = processor.validate(clean, processed, artifacts)
            metrics.processing_time = time.time() - start_time
            metrics.memory_usage = (
                psutil.Process().memory_info().rss - start_mem
            ) / 1024 / 1024  # MB
            
            results[artifact_type].append(metrics)
    
    return results

# From stemprover\analysis\artifacts\high_freq.py
class HighFrequencyArtifactPreprocessor(nn.Module):
    def __init__(self, threshold_freq: float = 11000, sample_rate: int = 44100):
        super().__init__()
        self.threshold_freq = threshold_freq
        self.sample_rate = sample_rate
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: RGBA spectrogram tensor (B, 4, H, W)
        Returns:
            Single-channel attention map (B, 1, H, W)
        """
        # Extract magnitude from blue channel
        magnitude = x[:, 2:3]  # (B, 1, H, W)
        
        # Calculate frequency bins
        freq_bins = torch.linspace(0, self.sample_rate/2, magnitude.shape[2])
        
        # Create frequency mask
        mask = torch.ones_like(magnitude)
        high_freq_idx = (freq_bins > self.threshold_freq).nonzero()
        
        if high_freq_idx.numel() > 0:
            # Analyze high frequency content
            high_freq_content = magnitude[:, :, high_freq_idx.squeeze():]
            
            # Detect potential artifacts using local statistics
            mean = torch.mean(high_freq_content, dim=-1, keepdim=True)
            std = torch.std(high_freq_content, dim=-1, keepdim=True)
            
            # Create attention map
            attention = torch.sigmoid(
                (high_freq_content - mean) / (std + 1e-6)
            )
            
            # Insert back into full mask
            mask[:, :, high_freq_idx.squeeze():] = attention
        
        return mask

def generate_training_pair(
    clean_audio: torch.Tensor,
    separated_audio: torch.Tensor,
    preprocessor: HighFrequencyArtifactPreprocessor
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Generate training pair for ControlNet
    Returns:
        condition: Control signal from preprocessor
        input_spec: Separated audio spectrogram
        target_spec: Clean audio spectrogram
    """
    # Convert both to spectrograms
    clean_spec = audio_to_spectrogram(clean_audio)
    sep_spec = audio_to_spectrogram(separated_audio)
    
    # Generate control signal
    condition = preprocessor(sep_spec)
    
    return condition, sep_spec, clean_spec

# From stemprover\analysis\artifacts\preprocessor.py
class HighFrequencyArtifactPreprocessor(nn.Module):
    def __init__(self, threshold_freq: float = 11000, sample_rate: int = 44100):
        super().__init__()
        self.threshold_freq = threshold_freq
        self.sample_rate = sample_rate
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: RGBA spectrogram tensor (B, 4, H, W)
        Returns:
            Single-channel attention map (B, 1, H, W)
        """
        # Extract magnitude from blue channel
        magnitude = x[:, 2:3]  # (B, 1, H, W)
        
        # Calculate frequency bins
        freq_bins = torch.linspace(0, self.sample_rate/2, magnitude.shape[2])
        
        # Create frequency mask
        mask = torch.ones_like(magnitude)
        high_freq_idx = (freq_bins > self.threshold_freq).nonzero()
        
        if high_freq_idx.numel() > 0:
            # Analyze high frequency content
            high_freq_content = magnitude[:, :, high_freq_idx.squeeze():]
            
            # Detect potential artifacts using local statistics
            mean = torch.mean(high_freq_content, dim=-1, keepdim=True)
            std = torch.std(high_freq_content, dim=-1, keepdim=True)
            
            # Create attention map
            attention = torch.sigmoid(
                (high_freq_content - mean) / (std + 1e-6)
            )
            
            # Insert back into full mask
            mask[:, :, high_freq_idx.squeeze():] = attention
        
        return mask

def generate_training_pair(
    clean_audio: torch.Tensor,
    separated_audio: torch.Tensor,
    preprocessor: HighFrequencyArtifactPreprocessor
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Generate training pair for ControlNet
    Returns:
        condition: Control signal from preprocessor
        input_spec: Separated audio spectrogram
        target_spec: Clean audio spectrogram
    """
    # Convert both to spectrograms
    clean_spec = audio_to_spectrogram(clean_audio)
    sep_spec = audio_to_spectrogram(separated_audio)
    
    # Generate control signal
    condition = preprocessor(sep_spec)
    
    return condition, sep_spec, clean_spec

# From stemprover\analysis\artifacts\spectral.py
from pathlib import Path
import json
import matplotlib.pyplot as plt
from typing import Dict
from datetime import datetime

"""from ...common.types import (  # Original relative import"""
    AudioArray, SpectrogramArray, FrequencyBands,
    DEFAULT_FREQUENCY_BANDS
)
"""from ...common.audio_utils import (  # Original relative import"""
    create_spectrogram, get_frequency_bins, get_band_mask
)
"""from ...common.math_utils import (  # Original relative import"""
    magnitude, angle, phase_difference, phase_coherence,
    rms, db_scale
)
"""from ...core.audio import AudioSegment  # Original relative import"""
"""from ...core.types import ProcessingConfig  # Original relative import"""

class SpectralAnalyzer:
    """Spectral analysis and visualization with standardized types"""
    
    def __init__(self, 
                 output_dir: Path, 
                 config: Optional[ProcessingConfig] = None,
                 frequency_bands: Optional[FrequencyBands] = None):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.config = config or ProcessingConfig()
        self.frequency_bands = frequency_bands or DEFAULT_FREQUENCY_BANDS
        self.normalization_params = {}
        
    def analyze(self, clean: AudioSegment, separated: AudioSegment) -> Path:
        """Perform spectral analysis"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_dir = self.output_dir / f"analysis_{timestamp}"
        analysis_dir.mkdir(exist_ok=True)
        
        clean_spec = self._create_spectrogram(clean.audio, clean.sample_rate)
        sep_spec = self._create_spectrogram(separated.audio, separated.sample_rate)
        
        self._save_comparison(
            clean_spec, 
            sep_spec,
            analysis_dir / "spectrogram_comparison.png"
        )
        
        diff_analysis = self._analyze_differences(clean_spec, sep_spec)
        self._save_analysis(diff_analysis, analysis_dir / "analysis.json")
        
        return analysis_dir

    def _create_spectrogram(self, audio: AudioArray, sr: int) -> SpectrogramArray:
        """Create spectrogram with phase preservation"""
        return create_spectrogram(
            audio,
            n_fft=self.config.n_fft,
            hop_length=self.config.hop_length
        )
        
    def _save_comparison(self, 
                        spec1: SpectrogramArray, 
                        spec2: SpectrogramArray, 
                        path: Path) -> None:
        """Save visual comparison of spectrograms"""
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 16))
        
        self._plot_spectrogram(
            spec1, 
            ax1, 
            "Clean Vocal Spectrogram",
            self.config.sample_rate
        )
        
        self._plot_spectrogram(
            spec2, 
            ax2, 
            "Separated Vocal Spectrogram",
            self.config.sample_rate
        )
        
        # Plot difference
        difference = db_scale(spec2) - db_scale(spec1)
        self._plot_spectrogram(
            difference,
            ax3,
            "Difference (Separated - Clean)",
            self.config.sample_rate,
            cmap='RdBu'
        )
        
        plt.tight_layout()
        plt.savefig(path)
        plt.close()
    
    def _plot_spectrogram(self,
                         spec: SpectrogramArray,
                         ax: plt.Axes,
                         title: str,
                         sr: int,
                         cmap: str = 'magma') -> None:
        """Helper to plot a single spectrogram"""
        librosa.display.specshow(
            db_scale(spec),
            y_axis='mel',
            x_axis='time',
            sr=sr,
            hop_length=self.config.hop_length,
            ax=ax,
            cmap=cmap
        )
        ax.set_title(title, color='white')
        ax.set_xlabel('Time (s)', color='white')
        ax.set_ylabel('Frequency (Hz)', color='white')
        ax.tick_params(colors='white')
        plt.colorbar(ax.collections[0], ax=ax, format='%+2.0f dB')
    
    def _analyze_differences(self, 
                           clean_spec: SpectrogramArray, 
                           separated_spec: SpectrogramArray) -> Dict:
        """Analyze spectral differences between clean and separated audio"""
        freq_bins = get_frequency_bins(
            sr=self.config.sample_rate,
            n_fft=self.config.n_fft
        )
        
        analysis = {}
        
        for band_name, (low_freq, high_freq) in self.frequency_bands.items():
            band_mask = get_band_mask(freq_bins, low_freq, high_freq)
            
            # Extract band data
            clean_band = clean_spec[band_mask]
            sep_band = separated_spec[band_mask]
            
            # Compute metrics
            mag_diff = magnitude(sep_band - clean_band).mean()
            phase_diff = phase_difference(clean_band, sep_band)
            
            analysis[band_name] = {
                "magnitude_difference": float(mag_diff),
                "phase_coherence": phase_coherence(phase_diff),
                "energy_ratio": rms(sep_band) / rms(clean_band) if rms(clean_band) > 0 else 0
            }
        
        # Overall metrics
        overall_phase_diff = phase_difference(clean_spec, separated_spec)
        analysis["overall"] = {
            "total_magnitude_difference": float(magnitude(separated_spec - clean_spec).mean()),
            "average_phase_coherence": phase_coherence(overall_phase_diff),
            "total_energy_ratio": rms(separated_spec) / rms(clean_spec)
        }
        
        return analysis
    
    def _save_analysis(self, analysis: Dict, path: Path) -> None:
        """Save analysis results as JSON"""
        with open(path, 'w') as f:
            json.dump(analysis, f, indent=2)

# From stemprover\analysis\selection\metrics.py
from dataclasses import dataclass

@dataclass
class SegmentMetrics:
    """Metrics for evaluating segment suitability"""
    vocal_clarity: float      # Ratio of vocal band energy to total
    high_freq_content: float  # Energy above 11kHz
    dynamic_range: float      # dB difference between peaks
    phase_complexity: float   # Measure of phase relationships
    transition_score: float   # Score for vocal transitions
    score: float = 0.0       # Overall suitability score

# From stemprover\analysis\selection\segment_finder.py
from dataclasses import dataclass
from typing import List, Dict
import numpy as np
import librosa

from common.types import AudioArray, SpectrogramArray
from common.audio_utils import create_spectrogram, calculate_onset_variation
from common.spectral_utils import calculate_band_energy
from core.types import ProcessingConfig
"""from .metrics import SegmentMetrics  # Original relative import"""

class TestSegmentFinder:
    """Finds ideal segments for overfitting tests"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.vocal_bands = (200, 4000)  # Primary vocal frequency range
        self.high_freq = 11000          # Spleeter cutoff
        
    def analyze_segment(self, 
                       vocal: AudioArray,
                       backing: AudioArray) -> SegmentMetrics:
        """Compute comprehensive metrics for a segment"""
        
        # Create spectrograms
        vocal_spec = create_spectrogram(
            vocal,
            n_fft=self.config.n_fft,
            hop_length=self.config.hop_length
        )
        mix_spec = create_spectrogram(
            vocal + backing,
            n_fft=self.config.n_fft,
            hop_length=self.config.hop_length
        )
        
        # Get frequency bins
        freqs = librosa.fft_frequencies(
            sr=self.config.sample_rate,
            n_fft=self.config.n_fft
        )
        
        # Calculate metrics
        vocal_clarity = self._calculate_vocal_clarity(
            vocal_spec, mix_spec, freqs
        )
        
        high_freq_content = self._calculate_high_freq_content(
            vocal_spec, freqs
        )
        
        dynamic_range = self._calculate_dynamic_range(vocal)
        
        phase_complexity = self._calculate_phase_complexity(
            vocal_spec, mix_spec
        )
        
        transition_score = self._calculate_transitions(vocal)
        
        # Compute overall score
        score = self._compute_score(
            vocal_clarity,
            high_freq_content,
            dynamic_range,
            phase_complexity,
            transition_score
        )
        
        return SegmentMetrics(
            vocal_clarity=vocal_clarity,
            high_freq_content=high_freq_content,
            dynamic_range=dynamic_range,
            phase_complexity=phase_complexity,
            transition_score=transition_score,
            score=score
        )
    
    def _calculate_vocal_clarity(self,
                               vocal_spec: SpectrogramArray,
                               mix_spec: SpectrogramArray,
                               freqs: np.ndarray) -> float:
        """Calculate vocal band clarity using band energy ratio"""
        vocal_energy = calculate_band_energy(
            vocal_spec, 
            freqs, 
            self.vocal_bands
        )
        mix_energy = calculate_band_energy(
            mix_spec,
            freqs,
            self.vocal_bands
        )
        return vocal_energy / (mix_energy + 1e-8)
    
    def _calculate_high_freq_content(self,
                                   spec: SpectrogramArray,
                                   freqs: np.ndarray) -> float:
        """Calculate high frequency content using band energy"""
        return calculate_band_energy(
            spec,
            freqs,
            (self.high_freq, freqs.max())
        )
    
    def _calculate_transitions(self, audio: AudioArray) -> float:
        """Score vocal transitions and variations"""
        return calculate_onset_variation(
            audio,
            self.config.sample_rate
        )

    def _compute_score(self,
                      vocal_clarity: float,
                      high_freq_content: float,
                      dynamic_range: float,
                      phase_complexity: float,
                      transition_score: float) -> float:
        """Compute overall suitability score"""
        # Weights for different criteria
        weights = {
            'baseline': {
                'vocal_clarity': 0.4,
                'high_freq_content': 0.3,
                'dynamic_range': 0.1,
                'phase_complexity': 0.1,
                'transition_score': 0.1
            },
            'challenge': {
                'vocal_clarity': 0.3,
                'high_freq_content': 0.2,
                'dynamic_range': 0.2,
                'phase_complexity': 0.15,
                'transition_score': 0.15
            },
            'complex': {
                'vocal_clarity': 0.2,
                'high_freq_content': 0.2,
                'dynamic_range': 0.2,
                'phase_complexity': 0.2,
                'transition_score': 0.2
            }
        }
        
        # Calculate scores for each test case type
        scores = {}
        for test_type, w in weights.items():
            scores[test_type] = (
                vocal_clarity * w['vocal_clarity'] +
                high_freq_content * w['high_freq_content'] +
                dynamic_range * w['dynamic_range'] +
                phase_complexity * w['phase_complexity'] +
                transition_score * w['transition_score']
            )
            
        # Return highest score
        return max(scores.values())

def find_best_segments(
    vocal_track: AudioArray,
    backing_track: AudioArray,
    segment_length: int,
    hop_length: int,
    config: ProcessingConfig,
    top_k: int = 5
) -> List[Dict]:
    """Find the best segments for testing"""
    finder = TestSegmentFinder(config)
    segments = []
    
    for start in range(0, len(vocal_track) - segment_length, hop_length):
        end = start + segment_length
        
        vocal_segment = vocal_track[start:end]
        backing_segment = backing_track[start:end]
        
        metrics = finder.analyze_segment(vocal_segment, backing_segment)
        
        segments.append({
            'start': start,
            'end': end,
            'metrics': metrics,
            'time': start / config.sample_rate
        })
    
    # Sort by score and return top_k
    segments.sort(key=lambda x: x['metrics'].score, reverse=True)
    return segments[:top_k]

# From stemprover\analysis\selection\__init__.py


# From stemprover\common\audio_utils.py
import numpy as np
import librosa
import soundfile as sf
"""from .types import AudioArray, SpectrogramArray, FrequencyBands  # Original relative import"""
"""from .math_utils import magnitude, angle, phase_difference, phase_coherence, rms  # Original relative import"""

def to_mono(audio: AudioArray) -> AudioArray:
    """Convert audio to mono if stereo"""
    if len(audio.shape) == 1:
        return audio
    return librosa.to_mono(audio.T)

def create_spectrogram(audio: AudioArray, **stft_params) -> SpectrogramArray:
    """Create spectrogram with standard parameters"""
    return librosa.stft(audio, **stft_params)

def get_frequency_bins(sr: int, n_fft: int) -> np.ndarray:
    """Get frequency bins for STFT"""
    return librosa.fft_frequencies(sr=sr, n_fft=n_fft)

def get_band_mask(freq_bins: np.ndarray, low_freq: float, high_freq: float) -> np.ndarray:
    """Get boolean mask for frequency band"""
    return (freq_bins >= low_freq) & (freq_bins <= high_freq)

def calculate_dynamic_range(self, audio: AudioArray) -> float:
    """Calculate dynamic range in dB"""
    # Use RMS with small windows
    frame_length = 2048
    hop_length = 512
        
    rms = librosa.feature.rms(
        y=audio,
        frame_length=frame_length,
        hop_length=hop_length
    )
        
    db_range = librosa.amplitude_to_db(rms.max()) - librosa.amplitude_to_db(rms.min())
    return float(db_range)
    
def calculate_phase_complexity(self,
                              vocal_spec: SpectrogramArray,
                              mix_spec: SpectrogramArray) -> float:
    """Measure complexity of phase relationships"""
    vocal_phase = np.angle(vocal_spec)
    mix_phase = np.angle(mix_spec)
        
    # Calculate phase differences and their variation
    phase_diff = np.abs(vocal_phase - mix_phase)
    return float(np.std(phase_diff))
    
def calculate_onset_variation(
    audio: AudioArray,
    sample_rate: int,
    normalize: bool = True
) -> float:
    """
    Calculate variation in onset strength as a measure of transitions.
    
    Args:
        audio: Input audio array
        sample_rate: Audio sample rate
        normalize: Whether to normalize the variation score
        
    Returns:
        Float indicating amount of transition variation
    """
    onset_env = librosa.onset.onset_strength(
        y=audio,
        sr=sample_rate
    )
    
    variation = np.std(onset_env)
    
    if normalize:
        # Normalize to 0-1 range based on typical values
        variation = variation / (variation + 1.0)
        
    return float(variation)

# From stemprover\common\math_utils.py
import numpy as np
"""from .types import AudioArray, SpectrogramArray  # Original relative import"""

def angle(complex_spec: SpectrogramArray) -> SpectrogramArray:
    """Get phase angle from complex spectrogram"""
    return np.angle(complex_spec)

def magnitude(complex_spec: SpectrogramArray) -> SpectrogramArray:
    """Get magnitude from complex spectrogram"""
    return np.abs(complex_spec)

def phase_difference(spec1: SpectrogramArray, spec2: SpectrogramArray) -> SpectrogramArray:
    """Compute phase difference between spectrograms"""
    return np.abs(angle(spec1) - angle(spec2))

def phase_coherence(phase_diff: SpectrogramArray) -> float:
    """Compute phase coherence from phase difference"""
    return float(np.mean(np.cos(phase_diff)))

def rms(array: AudioArray) -> float:
    """Compute root mean square"""
    return float(np.sqrt(np.mean(array ** 2)))

def db_scale(spec: SpectrogramArray, ref: float = None) -> SpectrogramArray:
    """Convert to dB scale"""
    return librosa.amplitude_to_db(magnitude(spec), ref=ref)

# From stemprover\common\spectral_utils.py
import numpy as np
import librosa
import soundfile as sf
from typing import Tuple
"""from .types import AudioArray, SpectrogramArray, FrequencyBand  # Original relative import"""
"""from .audio_utils import get_band_mask  # Original relative import"""

def calculate_band_energy(
    spec: SpectrogramArray,
    freqs: np.ndarray,
    band: Tuple[float, float],
    relative: bool = True
) -> float:
    """Calculate energy in specific frequency band"""
    band_mask = get_band_mask(freqs, band[0], band[1])
    band_energy = np.mean(np.abs(spec[band_mask]))
    
    if relative:
        total_energy = np.mean(np.abs(spec))
        return band_energy / (total_energy + 1e-8)
    return band_energy

# From stemprover\common\types.py
from typing import Union, List, Dict, Optional, Tuple, Any
import numpy as np
import torch
import librosa


# Common type aliases
AudioArray = np.ndarray
SpectrogramArray = np.ndarray
TensorType = Union[torch.Tensor, np.ndarray]
FrequencyBand = Tuple[float, float]
FrequencyBands = Dict[str, FrequencyBand]


# Common constants
DEFAULT_FREQUENCY_BANDS: FrequencyBands = {
    "sub_bass": (20, 60),
    "bass": (60, 250),
    "low_mid": (250, 500),
    "mid": (500, 2000),
    "high_mid": (2000, 4000),
    "presence": (4000, 6000),
    "brilliance": (6000, 20000)
}

# From stemprover\common\__init__.py
"""from .types import (  # Original relative import"""
    AudioArray, SpectrogramArray, TensorType, 
    FrequencyBand, FrequencyBands, DEFAULT_FREQUENCY_BANDS
)
"""from .audio_utils import (  # Original relative import"""
    to_mono, create_spectrogram, get_frequency_bins, get_band_mask
)
"""from .math_utils import (  # Original relative import"""
    angle, magnitude, phase_difference, phase_coherence, rms, db_scale
)
"""from .spectral_utils import (  # Original relative import"""
    calculate_band_energy
)

# From stemprover\core\audio.py
from dataclasses import dataclass
import numpy as np
import librosa
from typing import Optional

@dataclass
class AudioSegment:
    """Data class for audio segments with their metadata"""
    audio: np.ndarray
    sample_rate: int = 44100
    start_time: float = 0.0
    duration: float = 0.0

    @property
    def is_stereo(self) -> bool:
        stereo = len(self.audio.shape) == 2 and (
            self.audio.shape[0] == 2 or self.audio.shape[1] == 2
        )
        print(f"is_stereo check - shape: {self.audio.shape}, result: {stereo}")
        return stereo

    @property
    def is_mono(self) -> bool:
        mono = len(self.audio.shape) == 1 or (
            len(self.audio.shape) == 2 and (
                self.audio.shape[0] == 1 or self.audio.shape[1] == 1
            )
        )
        print(f"is_mono check - shape: {self.audio.shape}, result: {mono}")
        return mono

    def to_mono(self) -> 'AudioSegment':
        """Convert to mono if stereo"""
        print(f"to_mono - input shape: {self.audio.shape}")
        
        if self.is_mono:
            print("Already mono, returning as is")
            return self
            
        # Handle different stereo formats
        if len(self.audio.shape) == 2:
            if self.audio.shape[0] == 2:
                # (2, samples) format
                mono_audio = librosa.to_mono(self.audio)
            elif self.audio.shape[1] == 2:
                # (samples, 2) format
                mono_audio = librosa.to_mono(self.audio.T)
            else:
                raise ValueError(f"Unexpected audio shape: {self.audio.shape}")
        else:
            raise ValueError(f"Cannot convert shape {self.audio.shape} to mono")
            
        print(f"to_mono - output shape: {mono_audio.shape}")
        
        return AudioSegment(
            audio=mono_audio,
            sample_rate=self.sample_rate,
            start_time=self.start_time,
            duration=self.duration
        )

    @property
    def duration_seconds(self) -> float:
        """Get duration in seconds based on audio shape and sample rate"""
        if self.is_stereo:
            return self.audio.shape[1] / self.sample_rate
        return len(self.audio) / self.sample_rate

# From stemprover\core\config.py
from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
from typing import Optional

class SeparatorBackend(Enum):
    SPLEETER = auto()
    DEMUCS = auto()
    MDX = auto()

@dataclass
class SeparationProfile:
    """Processing profile configuration"""
    backend: SeparatorBackend
    preserve_high_freq: bool = False
    target_sample_rate: int = 44100
    min_segment_length: float = 5.0
    
    # Enhancement settings
    use_phase_aware_controlnet: bool = False
    use_high_freq_processor: bool = True
    artifact_reduction_config: Optional[ProcessingConfig] = None
    controlnet_model_path: Optional[Path] = None

# From stemprover\core\types.py
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Any
import matplotlib.pyplot as plt
"""from .audio import AudioSegment  # Original relative import"""

@dataclass
class ProcessingConfig:
    """Configuration for audio processing"""
    # Audio parameters
    sample_rate: int = 44100
    n_fft: int = 2048
    hop_length: int = 512
    pad_mode: str = 'constant'
    
    # Image processing
    image_scale_factor: float = 1.0
    image_chunk_size: int = 512
    
    # ControlNet parameters
    enable_controlnet: bool = True
    controlnet_strength: float = 0.75
    guidance_scale: float = 7.5
    num_inference_steps: int = 50
    
    # Optimization
    torch_dtype: str = 'float16'
    attention_slice_size: Optional[int] = 1
    enable_xformers: bool = True
    enable_cuda_graph: bool = False
    
    # Artifact processing
    artifact_threshold_freq: float = 11000
    artifact_smoothing_sigma: float = 1.0
    temporal_smoothing: float = 2.0

@dataclass
class SeparationResult:
    """Data class for storing separation results"""
    clean_vocal: AudioSegment
    separated_vocal: AudioSegment
    enhanced_vocal: Optional[AudioSegment]
    accompaniment: AudioSegment
    mixed: AudioSegment
    file_paths: Dict[str, Path]
    analysis_path: Optional[Path] = None
    phase_analysis: Optional[Dict[str, Any]] = None
    artifact_analysis: Optional[Dict[str, str]] = None

@dataclass
class SegmentConfig:
    """Configuration for segment generation"""
    segment_length: float = 5.0
    overlap: float = 2.5
    min_vocal_energy: float = 0.1  # Threshold for keeping vocal segments
    sample_rate: int = 44100
    
    @property
    def segment_samples(self) -> int:
        return int(self.segment_length * self.sample_rate)
    
    @property
    def hop_samples(self) -> int:
        return int((self.segment_length - self.overlap) * self.sample_rate)

# From stemprover\enhancement\base.py
from abc import ABC, abstractmethod
from typing import Optional
"""from ...core.audio import AudioSegment  # Original relative import"""
"""from ...core.types import ProcessingConfig  # Original relative import"""

class EnhancementProcessor(ABC):
    """Base class for audio enhancement processors"""
    
    def __init__(self, config: Optional[ProcessingConfig] = None):
        self.config = config or ProcessingConfig()
    
    @abstractmethod
    def enhance(self, audio: AudioSegment) -> AudioSegment:
        """Enhance audio segment"""
        pass
    
    @abstractmethod
    def validate(self, audio: AudioSegment) -> dict:
        """Validate enhancement results"""
        pass

# From stemprover\enhancement\controlnet.py
import torch
import torch.nn as nn
from typing import Optional, List, Tuple

class ArtifactDetector(nn.Module):
    """Preprocessor that generates artifact control maps"""
    def __init__(self):
        super().__init__()
        # Lightweight conv layers for artifact detection
        self.detector = nn.Sequential(
            nn.Conv2d(4, 16, 3, padding=1),  # 4 channels: RGB + phase
            nn.ReLU(),
            nn.Conv2d(16, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 1, 1)  # Single channel artifact map
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: RGBA tensor where A channel contains phase information
        Returns:
            Single-channel artifact probability map
        """
        return torch.sigmoid(self.detector(x))

class PhaseAwareZeroConv(nn.Module):
    """Zero convolution block that preserves phase information"""
    def __init__(
        self, 
        input_channels: int,
        output_channels: int,
        phase_channels: int = 1
    ):
        super().__init__()
        self.main_conv = nn.Conv2d(input_channels, output_channels, 1)
        self.phase_conv = nn.Conv2d(phase_channels, phase_channels, 1)
        
        # Initialize to zero
        nn.init.zeros_(self.main_conv.weight)
        nn.init.zeros_(self.main_conv.bias)
        nn.init.zeros_(self.phase_conv.weight)
        nn.init.zeros_(self.phase_conv.bias)
        
    def forward(self, x: torch.Tensor, control: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # Split main features and phase
        features, phase = x[:, :-1], x[:, -1:]
        
        # Apply zero convolutions
        out_features = self.main_conv(features * control)
        out_phase = self.phase_conv(phase * control)
        
        return torch.cat([out_features, out_phase], dim=1)

class PhaseAwareControlNet(nn.Module):
    """ControlNet adaptation for phase-aware spectrogram processing"""
    def __init__(
        self,
        base_model: nn.Module,
        control_channels: int = 1,
        phase_channels: int = 1
    ):
        super().__init__()
        self.base_model = base_model
        self.artifact_detector = ArtifactDetector()
        
        # Create zero conv layers for each injection point
        self.zero_convs = nn.ModuleList([
            PhaseAwareZeroConv(
                base_model.get_feature_channels(i),
                base_model.get_feature_channels(i),
                phase_channels
            )
            for i in range(base_model.num_injection_points)
        ])
        
    def forward(
        self, 
        x: torch.Tensor,
        timestep: Optional[torch.Tensor] = None,
        context: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        # Generate control signal
        control = self.artifact_detector(x)
        
        # Get intermediate features from frozen base model
        features = self.base_model.get_feature_pyramid(x, timestep, context)
        
        # Apply controlled features through zero convs
        controlled_features = []
        for feat, zero_conv in zip(features, self.zero_convs):
            controlled_features.append(zero_conv(feat, control))
            
        return self.base_model.forward_with_features(
            x,
            controlled_features,
            timestep,
            context
        )

# From stemprover\enhancement\training.py
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

class ArtifactDataset(Dataset):
    def __init__(self, 
                 clean_paths: List[str],
                 separated_paths: List[str],
                 preprocessor: HighFrequencyArtifactPreprocessor):
        self.clean_paths = clean_paths
        self.separated_paths = separated_paths
        self.preprocessor = preprocessor
        
    def __len__(self):
        return len(self.clean_paths)
        
    def __getitem__(self, idx):
        clean = load_audio(self.clean_paths[idx])
        separated = load_audio(self.separated_paths[idx])
        
        condition, input_spec, target_spec = generate_training_pair(
            clean, separated, self.preprocessor
        )
        
        return {
            'condition': condition,
            'input': input_spec,
            'target': target_spec
        }

class ControlNetTrainer:
    def __init__(self,
                 model: PhaseAwareControlNet,
                 preprocessor: HighFrequencyArtifactPreprocessor,
                 learning_rate: float = 1e-4,
                 device: str = 'cuda'):
        self.model = model.to(device)
        self.preprocessor = preprocessor.to(device)
        self.device = device
        
        # Freeze base model, train only ControlNet components
        for param in self.model.base_model.parameters():
            param.requires_grad = False
            
        self.optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=learning_rate
        )
        
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        self.optimizer.zero_grad()
        
        # Move data to device
        condition = batch['condition'].to(self.device)
        input_spec = batch['input'].to(self.device)
        target_spec = batch['target'].to(self.device)
        
        # Forward pass
        output = self.model(input_spec, control=condition)
        
        # Compute losses
        losses = {
            'magnitude': F.mse_loss(output[:, :-1], target_spec[:, :-1]),
            'phase': F.mse_loss(output[:, -1:], target_spec[:, -1:]),
            'frequency': self.frequency_loss(output, target_spec)
        }
        
        # Combined loss
        total_loss = sum(losses.values())
        total_loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            max_norm=1.0
        )
        
        self.optimizer.step()
        
        return {k: v.item() for k, v in losses.items()}
    
    def frequency_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """Frequency-domain loss focusing on high frequencies"""
        # Convert to frequency domain
        pred_fft = torch.fft.rfft2(pred[:, :-1])  # Exclude phase channel
        target_fft = torch.fft.rfft2(target[:, :-1])
        
        # Weight higher frequencies more
        freq_weights = torch.linspace(1.0, 2.0, pred_fft.size(-1))
        freq_weights = freq_weights.to(self.device)
        
        return F.mse_loss(
            pred_fft * freq_weights,
            target_fft * freq_weights
        )
    
    def train(self,
             train_loader: DataLoader,
             val_loader: Optional[DataLoader] = None,
             epochs: int = 100,
             save_dir: str = 'checkpoints'):
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            self.model.train()
            train_losses = []
            
            for batch in train_loader:
                losses = self.train_step(batch)
                train_losses.append(losses)
            
            # Validation
            if val_loader is not None:
                val_loss = self.validate(val_loader)
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    self.save_checkpoint(f'{save_dir}/best.pt')
            
            if epoch % 10 == 0:
                self.save_checkpoint(f'{save_dir}/epoch_{epoch}.pt')
    
    def validate(self, val_loader: DataLoader) -> float:
        self.model.eval()
        val_losses = []
        
        with torch.no_grad():
            for batch in val_loader:
                condition = batch['condition'].to(self.device)
                input_spec = batch['input'].to(self.device)
                target_spec = batch['target'].to(self.device)
                
                output = self.model(input_spec, control=condition)
                loss = F.mse_loss(output, target_spec)
                val_losses.append(loss.item())
        
        return sum(val_losses) / len(val_losses)
    
    def save_checkpoint(self, path: str):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict()
        }, path)
    
    def load_checkpoint(self, path: str):
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

def prepare_training(
    clean_dir: str,
    separated_dir: str,
    batch_size: int = 8,
    val_split: float = 0.1
) -> Tuple[DataLoader, DataLoader]:
    # Get audio paths
    clean_paths = sorted(glob.glob(f'{clean_dir}/*.wav'))
    separated_paths = sorted(glob.glob(f'{separated_dir}/*.wav'))
    
    # Split train/val
    split_idx = int(len(clean_paths) * (1 - val_split))
    
    # Create datasets
    preprocessor = HighFrequencyArtifactPreprocessor()
    train_dataset = ArtifactDataset(
        clean_paths[:split_idx],
        separated_paths[:split_idx],
        preprocessor
    )
    val_dataset = ArtifactDataset(
        clean_paths[split_idx:],
        separated_paths[split_idx:],
        preprocessor
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4
    )
    
    return train_loader, val_loader

# From stemprover\io\audio.py
from pathlib import Path
import numpy as np
import soundfile as sf
import librosa
from typing import Tuple
"""from ..core.audio import AudioSegment  # Original relative import"""

def load_audio_file(path: str, sr: int = 44100, mono: bool = False) -> Tuple[np.ndarray, int]:
    """Load audio file with error handling and validation"""
    try:
        audio, file_sr = librosa.load(path, sr=sr, mono=mono)
        return audio, file_sr
    except Exception as e:
        raise RuntimeError(f"Error loading audio file {path}: {str(e)}")

def save_audio_file(audio: AudioSegment, path: Path) -> None:
    """Save audio file with proper format handling"""
    try:
        # Handle different array shapes
        audio_to_save = audio.audio
        if audio.is_stereo:
            # Convert from (2, samples) to (samples, 2)
            audio_to_save = audio.audio.T
            
        sf.write(str(path), audio_to_save, audio.sample_rate)
    except Exception as e:
        raise RuntimeError(f"Error saving audio file {path}: {str(e)}")

# From stemprover\preparation\base.py
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Tuple, Dict
"""from ..core.types import SeparationResult  # Original relative import"""
"""from ..core.audio import AudioSegment  # Original relative import"""

class VocalSeparator(ABC):
    """Abstract base class for vocal separators"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def separate_and_analyze(self,
                           vocal_paths: Tuple[str, str],
                           accompaniment_paths: Tuple[str, str],
                           start_time: float = 0.0,
                           duration: float = 30.0,
                           run_analysis: bool = True) -> SeparationResult:
        """Perform separation and analysis"""
        pass

    @abstractmethod
    def _load_stereo_pair(self, left_path: str, right_path: str, 
                         start_time: float, duration: float) -> AudioSegment:
        """Load and process stereo pair"""
        pass

    @abstractmethod
    def _separate_vocals(self, mixed: AudioSegment) -> AudioSegment:
        """Perform vocal separation"""
        pass

    @abstractmethod
    def _save_audio_files(self, vocals: AudioSegment, 
                         accompaniment: AudioSegment,
                         mixed: AudioSegment, 
                         separated: AudioSegment,
                         start_time: float) -> Dict[str, Path]:
        """Save all audio files"""
        pass

    def cleanup(self):
        """Cleanup resources - override if needed"""
        pass

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

# From stemprover\preparation\segments\generator.py
from pathlib import Path
from typing import List, Tuple, Dict, Generator
from dataclasses import dataclass
from torch.utils.data import Dataset, DataLoader
import numpy as np

from common.types import AudioArray
from common.audio_utils import create_spectrogram
from core.audio import AudioSegment
from core.types import ProcessingConfig

"""
Key features:
1. Configurable segment length and overlap
2. Filters out segments with insufficient vocal content
3. Creates multiple backing track combinations
4. Generates spectrograms for training
5. Preserves timing information for potential time-based analysis
"""

class TrainingSegmentGenerator:
    """Generates training segments from multitrack sources"""
    
    def __init__(self, 
                 config: SegmentConfig,
                 processing_config: ProcessingConfig):
        self.config = config
        self.processing_config = processing_config
        
    def generate_segments(self,
                         vocal_path: Path,
                         backing_paths: List[Path]) -> Generator[Dict, None, None]:
        """Generate training segments from a song's tracks"""
        
        # Load vocal track
        vocal = AudioSegment.from_file(vocal_path)
        
        # Load backing tracks
        backing_tracks = [AudioSegment.from_file(path) for path in backing_paths]
        
        # Generate segments
        for start_idx in range(0, len(vocal.audio), self.config.hop_samples):
            end_idx = start_idx + self.config.segment_samples
            
            if end_idx > len(vocal.audio):
                break
                
            # Extract vocal segment
            vocal_segment = vocal.audio[start_idx:end_idx]
            
            # Check if segment has sufficient vocal content
            if self._has_vocal_content(vocal_segment):
                # Create different backing track combinations
                backing_combinations = self._create_backing_combinations(
                    backing_tracks,
                    start_idx,
                    end_idx
                )
                
                # Generate training pairs for each combination
                for mix_name, backing_mix in backing_combinations.items():
                    # Create mixed audio
                    mixed = vocal_segment + backing_mix
                    
                    # Create spectrograms
                    vocal_spec = create_spectrogram(
                        vocal_segment,
                        n_fft=self.processing_config.n_fft,
                        hop_length=self.processing_config.hop_length
                    )
                    
                    mixed_spec = create_spectrogram(
                        mixed,
                        n_fft=self.processing_config.n_fft,
                        hop_length=self.processing_config.hop_length
                    )
                    
                    yield {
                        'clean': vocal_spec,
                        'mixed': mixed_spec,
                        'source_audio': mixed,
                        'target_audio': vocal_segment,
                        'mix_type': mix_name,
                        'start_time': start_idx / self.config.sample_rate,
                        'duration': self.config.segment_length
                    }
    
    def _has_vocal_content(self, segment: AudioArray) -> bool:
        """Check if segment has sufficient vocal energy"""
        rms = np.sqrt(np.mean(segment ** 2))
        return rms > self.config.min_vocal_energy
    
    def _create_backing_combinations(self,
                                   backing_tracks: List[AudioSegment],
                                   start_idx: int,
                                   end_idx: int) -> Dict[str, AudioArray]:
        """Create different combinations of backing tracks"""
        segments = [track.audio[start_idx:end_idx] for track in backing_tracks]
        
        # Create standard combinations
        combinations = {
            'full_band': sum(segments),
            'no_drums': sum(segments[:-1]) if len(segments) > 1 else segments[0],
        }
        
        # Add pairs of instruments if we have enough tracks
        if len(segments) >= 2:
            combinations['guitar_bass'] = segments[0] + segments[1]
            
        if len(segments) >= 3:
            combinations['rhythm_section'] = segments[1] + segments[2]  # bass + drums
            
        return combinations
"""
Usage would look like:

# Configuration
segment_config = SegmentConfig(
    segment_length=5.0,
    overlap=2.5,
    min_vocal_energy=0.1
)

processing_config = ProcessingConfig(
    sample_rate=44100,
    n_fft=2048,
    hop_length=512
)

# Example song paths
songs = [
    {
        'vocal': Path('song1/vocals.wav'),
        'guitar': Path('song1/guitar.wav'),
        'bass': Path('song1/bass.wav'),
        'drums': Path('song1/drums.wav')
    },
    # ... more songs
]

# Create dataset
dataset = TrainingDataset(songs, segment_config, processing_config)

# Use with DataLoader
dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=4,
    shuffle=True,
    num_workers=4
)
"""

# From stemprover\preparation\segments\__init__.py


# From stemprover\separation\base.py
from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
from typing import Dict, Optional, Tuple

@dataclass
class SeparationProfile:
    backend: SeparatorBackend
    preserve_high_freq: bool = False
    target_sample_rate: int = 44100
    min_segment_length: float = 5.0
    # Added enhancement parameters
    use_phase_aware_controlnet: bool = False
    use_high_freq_processor: bool = True
    artifact_reduction_config: Optional[ProcessingConfig] = None
    controlnet_model_path: Optional[Path] = None

@dataclass
class SeparationResult:
    clean_vocal: AudioSegment
    separated_vocal: AudioSegment
    enhanced_vocal: Optional[AudioSegment]  # After artifact reduction
    accompaniment: AudioSegment
    mixed: AudioSegment
    analysis_path: Optional[Path] = None
    phase_analysis: Optional[Dict[str, Any]] = None
    artifact_analysis: Optional[Dict[str, str]] = None

class StemProcessor:
    def __init__(self, profile: SeparationProfile):
        self.profile = profile
        self.separator = self._create_separator()
        self.analyzer = VocalSeparationAnalyzer(Path("analysis_output"))
        
        # Initialize enhancement components
        self.artifact_processor = IntegratedHighFrequencyProcessor(
            "enhancement_output",
            config=profile.artifact_reduction_config or ProcessingConfig()
        ) if profile.use_high_freq_processor else None
        
        self.controlnet = PhaseAwareControlNet.from_pretrained(
            profile.controlnet_model_path
        ) if profile.use_phase_aware_controlnet else None

    def process_stems(self, 
                     vocal_paths: Tuple[str, str],
                     accompaniment_paths: Tuple[str, str],
                     start_time: float = 0.0,
                     duration: float = 30.0) -> SeparationResult:
        try:
            self.separator.setup()
            
            # Load and prepare audio
            vocals = self._load_stereo_pair(*vocal_paths, start_time, duration)
            accompaniment = self._load_stereo_pair(*accompaniment_paths, start_time, duration)
            mixed = AudioSegment(
                audio=vocals.audio + accompaniment.audio,
                sample_rate=vocals.sample_rate,
                start_time=start_time,
                duration=duration
            )

            # Initial separation
            separated = self.separator.separate(mixed)
            base_analysis = self.analyzer.analyze(vocals, separated)
            
            # Enhancement pipeline
            enhanced = separated
            artifact_analysis = None
            
            # Apply high-frequency artifact reduction if enabled
            if self.artifact_processor:
                enhanced_audio, analysis_files = self.artifact_processor.process_and_analyze(
                    enhanced.audio
                )
                enhanced = AudioSegment(
                    audio=enhanced_audio,
                    sample_rate=enhanced.sample_rate,
                    start_time=enhanced.start_time,
                    duration=enhanced.duration
                )
                artifact_analysis = analysis_files
            
            # Apply ControlNet enhancement if enabled
            if self.controlnet:
                enhanced = self._apply_controlnet_enhancement(enhanced)
            
            return SeparationResult(
                clean_vocal=vocals,
                separated_vocal=separated,
                enhanced_vocal=enhanced,
                accompaniment=accompaniment,
                mixed=mixed,
                analysis_path=base_analysis,
                artifact_analysis=artifact_analysis
            )
        finally:
            self.separator.cleanup()

    def _apply_controlnet_enhancement(self, audio: AudioSegment) -> AudioSegment:
        # Convert to spectrogram
        spec = self.artifact_processor.create_complex_spectrogram(
            audio.audio
        ) if self.artifact_processor else None
        
        # Process through ControlNet
        enhanced_spec = self.controlnet(spec)
        
        # Convert back to audio
        enhanced_audio = self.artifact_processor.spectrogram_to_audio(
            enhanced_spec
        ) if self.artifact_processor else None
        
        return AudioSegment(
            audio=enhanced_audio,
            sample_rate=audio.sample_rate,
            start_time=audio.start_time,
            duration=audio.duration
        )

# From stemprover\separation\spleeter.py
import tensorflow as tf
from pathlib import Path
import numpy as np
from typing import Dict, Tuple, Optional
from datetime import datetime
from spleeter.separator import Separator as SpleeterBase

"""from .base import VocalSeparator  # Original relative import"""
"""from ..core.audio import AudioSegment  # Original relative import"""
"""from ..core.types import SeparationResult  # Original relative import"""
"""from ..io.audio import load_audio_file, save_audio_file  # Original relative import"""
"""from ..analysis.spectral import SpectralAnalyzer  # Original relative import"""

class SpleeterSeparator(VocalSeparator):
    """Concrete implementation using Spleeter"""
    
    def __init__(self, output_dir: str = "output"):
        super().__init__(output_dir)
        
        # Initialize components
        self.analyzer = SpectralAnalyzer(self.output_dir)
        
        # Defer TensorFlow setup until needed
        self.separator = None
        self.graph = None
        self.session = None

    def _setup_tensorflow(self):
        """Setup TensorFlow session and graph - called only when needed"""
        if self.separator is not None:
            return  # Already initialized
            
        # Create a new graph and session without resetting
        self.graph = tf.Graph()
        self.graph.as_default().__enter__()
        
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        self.session = tf.compat.v1.Session(graph=self.graph, config=config)
        self.session.as_default().__enter__()
        
        # Initialize Spleeter only after graph/session setup
        self.separator = SpleeterBase('spleeter:2stems')

    def separate_and_analyze(self,
                           vocal_paths: Tuple[str, str],
                           accompaniment_paths: Tuple[str, str],
                           start_time: float = 0.0,
                           duration: float = 30.0,
                           run_analysis: bool = True) -> SeparationResult:
        """Main method to perform separation and analysis"""
        # Ensure TensorFlow is set up
        self._setup_tensorflow()
        
        # Load audio
        vocals = self._load_stereo_pair(*vocal_paths, start_time, duration)
        accompaniment = self._load_stereo_pair(*accompaniment_paths, start_time, duration)

        # Create mix
        mixed = AudioSegment(
            audio=vocals.audio + accompaniment.audio,
            sample_rate=vocals.sample_rate,
            start_time=start_time,
            duration=duration
        )

        # Perform separation
        separated = self._separate_vocals(mixed)
        
        # Save files
        file_paths = self._save_audio_files(
            vocals, accompaniment, mixed, separated, start_time
        )

        result = SeparationResult(
            clean_vocal=vocals,
            separated_vocal=separated,
            accompaniment=accompaniment,
            mixed=mixed,
            file_paths=file_paths
        )

        # Run analysis if requested
        if run_analysis:
            result.analysis_path = self.analyzer.analyze(vocals, separated)

        return result

    def _load_stereo_pair(self, left_path: str, right_path: str, 
                         start_time: float, duration: float) -> AudioSegment:
        """Load and process stereo pair"""
        print(f"Loading {left_path}...")
        left, sr = load_audio_file(left_path, sr=44100, mono=True)
        print(f"Left channel length: {len(left)} samples ({len(left)/44100:.2f} seconds)")

        print(f"Loading {right_path}...")
        right, _ = load_audio_file(right_path, sr=44100, mono=True)
        print(f"Right channel length: {len(right)} samples ({len(right)/44100:.2f} seconds)")

        # Ensure same length
        min_length = min(len(left), len(right))
        left = left[:min_length]
        right = right[:min_length]
        print(f"Adjusted stereo length: {min_length} samples ({min_length/44100:.2f} seconds)")

        # Extract segment
        start_sample = int(start_time * 44100)
        duration_samples = int(duration * 44100)
        
        if start_sample + duration_samples > min_length:
            print(f"Warning: Requested duration extends beyond audio length. Truncating.")
            duration_samples = min_length - start_sample
        
        left_segment = left[start_sample:start_sample + duration_samples]
        right_segment = right[start_sample:start_sample + duration_samples]

        # Stack to stereo
        stereo = np.vstack([left_segment, right_segment])
        
        return AudioSegment(
            audio=stereo,
            sample_rate=44100,
            start_time=start_time,
            duration=duration_samples/44100
        )

    def _separate_vocals(self, mixed: AudioSegment) -> AudioSegment:
        """Perform vocal separation"""
        # Convert to mono and reshape for Spleeter
        mix_mono = mixed.to_mono().audio
        mix_mono = mix_mono.reshape(-1, 1)

        print(f"Mix shape before separation: {mix_mono.shape}")
        print("Running separation...")
        
        separated = self.separator.separate(mix_mono)
        separated_vocals = separated['vocals']
        print(f"Separated vocals shape: {separated_vocals.shape}")
        
        # Since Spleeter returns (samples, channels), we should handle it accordingly
        if len(separated_vocals.shape) == 2:
            if separated_vocals.shape[1] == 2:
                # If it's stereo, convert to our preferred format (2, samples)
                separated_vocals = separated_vocals.T
            elif separated_vocals.shape[1] == 1:
                # If it's mono in (samples, 1) shape, convert to 1D array
                separated_vocals = separated_vocals.reshape(-1)
                
        print(f"Final separated vocals shape: {separated_vocals.shape}")
        
        if len(separated_vocals.shape) == 1:
            # If mono, duplicate to stereo to match input
            separated_vocals = np.vstack([separated_vocals, separated_vocals])
        
        print(f"Output separated vocals shape: {separated_vocals.shape}")
            
        if separated_vocals.shape[1] < 1000:  # Sanity check on the correct dimension
            raise ValueError(f"Separated vocals seem too short: {separated_vocals.shape}")
                
        return AudioSegment(
            audio=separated_vocals,
            sample_rate=mixed.sample_rate,
            start_time=mixed.start_time,
            duration=mixed.duration
        )

    def _save_audio_files(self, vocals: AudioSegment, accompaniment: AudioSegment,
                         mixed: AudioSegment, separated: AudioSegment,
                         start_time: float) -> Dict[str, Path]:
        """Save all audio files"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = self.output_dir / f"separation_{timestamp}"
        save_dir.mkdir(exist_ok=True)

        files = {}
        
        # Prepare audio segments for saving
        segments = {
            'clean_vocal': vocals,
            'accompaniment': accompaniment,
            'mix': mixed,
            'separated_vocal': separated
        }

        for name, segment in segments.items():
            path = save_dir / f"{name}_from_{start_time:.1f}s.wav"
            print(f"\nProcessing {name}:")
            print(f"Original shape: {segment.audio.shape}")
            
            save_audio_file(segment, path)
            files[name] = path

        return files

    def cleanup(self):
        """Cleanup resources"""
        try:
            if hasattr(self, 'separator') and hasattr(self.separator, '_get_model'):
                self.separator._get_model.cache_clear()
            
            if hasattr(self, 'session'):
                self.session.as_default().__exit__(None, None, None)
                self.session.close()
                delattr(self, 'session')
            
            if hasattr(self, 'graph'):
                self.graph.as_default().__exit__(None, None, None)
                delattr(self, 'graph')
            
        except Exception as e:
            print(f"Warning during cleanup: {str(e)}")

    @property
    def capabilities(self) -> Dict[str, any]:
        """Report capabilities/limitations of this backend"""
        return {
            "max_frequency": 11000,  # Hz
            "supports_stereo": True,
            "native_sample_rate": 22050,
            "recommended_min_segment": 5.0  # seconds
        }

# From stemprover\training\dataset.py
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Path
"""from .core.types import ProcessingConfig, SegmentConfig  # Original relative import"""
"""from .preparation.segments import TrainingSegmentGenerator  # Original relative import"""
class TrainingDataset(Dataset):
    """Dataset for training with segmented audio"""
    
    def __init__(self,
                 song_paths: List[Dict[str, Path]],
                 segment_config: SegmentConfig,
                 processing_config: ProcessingConfig):
        self.generator = TrainingSegmentGenerator(segment_config, processing_config)
        self.segments = []
        
        # Pre-generate all segments
        for song in song_paths:
            segments = list(self.generator.generate_segments(
                song['vocal'],
                [song['guitar'], song['bass'], song['drums']]
            ))
            self.segments.extend(segments)
    
    def __len__(self) -> int:
        return len(self.segments)
    
    def __getitem__(self, idx: int) -> Dict:
        return self.segments[idx]

# From stemprover\training\pairs.py
